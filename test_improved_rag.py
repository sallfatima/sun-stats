#!/usr/bin/env python3
"""
Script de test pour valider les amÃ©liorations du systÃ¨me RAG ANSD.
Version 2 avec chargement automatique du fichier .env
"""

# =============================================================================
# CHARGEMENT DU FICHIER .env (PREMIÃˆRE CHOSE Ã€ FAIRE)
# =============================================================================
from dotenv import load_dotenv
load_dotenv()
print("âœ… Fichier .env chargÃ©")

import asyncio
import time
import sys
import os

# Ajouter le rÃ©pertoire src au path Python
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

# VÃ©rification des clÃ©s API aprÃ¨s chargement .env
def verify_api_keys():
    """VÃ©rifie que les clÃ©s API essentielles sont chargÃ©es."""
    print("\nğŸ” VÃ©rification des clÃ©s API:")
    
    api_keys = {
        'OPENAI_API_KEY': ('OpenAI', True),  # (description, required)
        'ANTHROPIC_API_KEY': ('Anthropic Claude', False),
        'PINECONE_API_KEY': ('Pinecone Vector DB', False),
        'LANGSMITH_API_KEY': ('LangSmith Tracing', False)
    }
    
    missing_required = []
    
    for key, (description, required) in api_keys.items():
        value = os.getenv(key)
        if value and value.strip():
            # Masquer les clÃ©s sensibles
            masked_value = value[:10] + '...' if len(value) > 10 else '***'
            print(f"   âœ… {description}: {masked_value}")
        else:
            if required:
                print(f"   âŒ {description}: MANQUANTE (REQUIS)")
                missing_required.append(key)
            else:
                print(f"   âšª {description}: Non configurÃ©e (optionnel)")
    
    if missing_required:
        print(f"\nğŸš¨ Erreur: ClÃ©s requises manquantes: {', '.join(missing_required)}")
        print("ğŸ’¡ VÃ©rifiez votre fichier .env")
        return False
    
    print("âœ… Toutes les clÃ©s requises sont configurÃ©es")
    return True

# VÃ©rifier les clÃ©s API
if not verify_api_keys():
    print("\nâ¹ï¸  ArrÃªt du script - Configuration API incomplÃ¨te")
    sys.exit(1)

# Imports du systÃ¨me RAG (aprÃ¨s vÃ©rification des clÃ©s)
try:
    from langchain_core.messages import HumanMessage
    print("âœ… langchain_core importÃ©")
    
    # Import direct du module sans passer par __init__.py pour Ã©viter les imports circulaires
    try:
        from simple_rag.graph import graph
        from simple_rag.configuration import RagConfiguration
        print("âœ… Modules RAG importÃ©s")
    except ImportError as e:
        print(f"âš ï¸  Import direct Ã©chouÃ©: {e}")
        # Essayer import alternatif
        sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src', 'simple_rag'))
        from graph import graph
        from configuration import RagConfiguration
        print("âœ… Modules RAG importÃ©s (mÃ©thode alternative)")

except ImportError as e:
    print(f"âŒ Erreur d'import critique: {e}")
    print("ğŸ’¡ VÃ©rifiez que toutes les dÃ©pendances sont installÃ©es:")
    print("   pip install langchain-core langgraph python-dotenv")
    sys.exit(1)

# Fonction de validation des rÃ©ponses (basique, sans dÃ©pendances externes)
def validate_ansd_response(response: str) -> dict:
    """Valide qu'une rÃ©ponse Ã©tendue contient les Ã©lÃ©ments requis pour l'ANSD."""
    
    validation = {
        'has_numerical_data': False,
        'has_source_citation': False,
        'has_year_reference': False,
        'has_ansd_terminology': False,
        'has_structure': False,
        'is_comprehensive': False,
        'has_external_ansd_knowledge': False,
        'quality_score': 0.0,
        'suggestions': []
    }
    
    response_lower = response.lower()
    
    # VÃ©rifier la prÃ©sence de donnÃ©es numÃ©riques
    import re
    if re.search(r'\d+(?:[.,]\d+)?(?:\s*%|\s*millions?|\s*milliards?|\s*habitants?)', response):
        validation['has_numerical_data'] = True
        validation['quality_score'] += 15
    else:
        validation['suggestions'].append("Ajouter des donnÃ©es chiffrÃ©es prÃ©cises")
    
    # VÃ©rifier les citations de sources ANSD (documents + publications)
    source_indicators = [
        'source :', 'page ', 'document', 'rgph', 'eds', 'esps', 'ehcvm', 'enes', 
        'ansd', 'recensement', 'enquÃªte', 'rapport', 'publication'
    ]
    if any(term in response_lower for term in source_indicators):
        validation['has_source_citation'] = True
        validation['quality_score'] += 15
    else:
        validation['suggestions'].append("Citer les sources ANSD spÃ©cifiques")
    
    # VÃ©rifier les rÃ©fÃ©rences temporelles
    if re.search(r'20\d{2}|annÃ©e\s+de\s+rÃ©fÃ©rence', response):
        validation['has_year_reference'] = True
        validation['quality_score'] += 15
    else:
        validation['suggestions'].append("PrÃ©ciser l'annÃ©e de rÃ©fÃ©rence des donnÃ©es")
    
    # VÃ©rifier la terminologie ANSD
    ansd_terms = ['statistique', 'dÃ©mographique', 'sÃ©nÃ©gal', 'mÃ©thodologie', 'indicateur']
    if any(term in response_lower for term in ansd_terms):
        validation['has_ansd_terminology'] = True
        validation['quality_score'] += 10
    else:
        validation['suggestions'].append("Utiliser la terminologie statistique appropriÃ©e")
    
    # VÃ©rifier la structure de la rÃ©ponse
    structure_markers = ['**rÃ©ponse directe**', '**donnÃ©es prÃ©cises**', '**contexte additionnel**', '-']
    if any(marker in response_lower for marker in structure_markers):
        validation['has_structure'] = True
        validation['quality_score'] += 15
    else:
        validation['suggestions'].append("AmÃ©liorer la structure de la rÃ©ponse")
    
    # VÃ©rifier le caractÃ¨re complet/dÃ©veloppÃ©
    if len(response) > 500:  # RÃ©ponse dÃ©veloppÃ©e
        validation['is_comprehensive'] = True
        validation['quality_score'] += 15
    else:
        validation['suggestions'].append("DÃ©velopper davantage la rÃ©ponse")
    
    # VÃ©rifier l'utilisation de connaissances ANSD externes
    external_indicators = [
        'selon les publications ansd', 'd\'aprÃ¨s les rapports ansd', 
        'site officiel ansd', 'www.ansd.sn', 'publications officielles'
    ]
    if any(indicator in response_lower for indicator in external_indicators):
        validation['has_external_ansd_knowledge'] = True
        validation['quality_score'] += 15
    else:
        # Pas de pÃ©nalitÃ© si pas de connaissances externes, mais bonus si prÃ©sentes
        pass
    
    return validation

class ANSDRAGTester:
    """Classe pour tester le systÃ¨me RAG amÃ©liorÃ© de l'ANSD."""
    
    def __init__(self):
        try:
            self.config = RagConfiguration()
            print("âœ… Configuration RAG initialisÃ©e")
        except Exception as e:
            print(f"âš ï¸  Erreur configuration RAG: {e}")
            # Configuration par dÃ©faut minimale
            self.config = type('Config', (), {
                'model': 'openai/gpt-4o',
                'retrieval_k': 15,
                'enable_debug_logs': True
            })()
            print("âœ… Configuration par dÃ©faut utilisÃ©e")
        
        self.test_questions = [
            "Quelle est la population du SÃ©nÃ©gal selon le dernier RGPH ?",
            "Quel est le taux de pauvretÃ© au SÃ©nÃ©gal ?",
            "Qu'est-ce que le recensement de la population ?",
            "Quels sont les indicateurs de santÃ© maternelle au SÃ©nÃ©gal ?",
            "Comment Ã©volue le taux d'alphabÃ©tisation au SÃ©nÃ©gal ?"
        ]
    
    async def test_single_question(self, question: str, verbose: bool = True):
        """Teste une question unique et retourne les rÃ©sultats."""
        
        if verbose:
            print(f"\nğŸ” TEST: {question}")
            print("=" * 80)
        
        start_time = time.time()
        
        try:
            # PrÃ©parer l'Ã©tat initial
            initial_state = {
                "messages": [HumanMessage(content=question)]
            }
            
            # Configuration pour le graphe
            if hasattr(self.config, '__dict__'):
                config_dict = {"configurable": self.config.__dict__}
            else:
                config_dict = {"configurable": self.config}
            
            # ExÃ©cuter le graphe RAG
            result = await graph.ainvoke(initial_state, config=config_dict)
            
            processing_time = time.time() - start_time
            
            # Extraire la rÃ©ponse
            if result and "messages" in result and result["messages"]:
                response = result["messages"][-1].content
                documents = result.get("documents", [])
                
                if verbose:
                    print(f"âœ… RÃ‰PONSE GÃ‰NÃ‰RÃ‰E ({processing_time:.2f}s):")
                    print("-" * 60)
                    print(response)
                    print("-" * 60)
                    print(f"ğŸ“š Documents utilisÃ©s: {len(documents)}")
                    
                    # Validation de la qualitÃ©
                    validation = validate_ansd_response(response)
                    print(f"\nğŸ“Š SCORE DE QUALITÃ‰: {validation['quality_score']}/100")
                    
                    # DÃ©tail des vÃ©rifications
                    checks = {
                        "DonnÃ©es numÃ©riques": validation['has_numerical_data'],
                        "Sources ANSD citÃ©es": validation['has_source_citation'],
                        "AnnÃ©e de rÃ©fÃ©rence": validation['has_year_reference'],
                        "Terminologie ANSD": validation['has_ansd_terminology'],
                        "Structure claire": validation['has_structure']
                    }
                    
                    print("ğŸ” VÃ©rifications dÃ©taillÃ©es:")
                    for check, passed in checks.items():
                        status = "âœ…" if passed else "âŒ"
                        print(f"   {status} {check}")
                    
                    if validation['suggestions']:
                        print("\nğŸ’¡ SUGGESTIONS D'AMÃ‰LIORATION:")
                        for suggestion in validation['suggestions']:
                            print(f"   â€¢ {suggestion}")
                    
                    # Ã‰valuation globale
                    if validation['quality_score'] >= 80:
                        print("\nğŸ‰ EXCELLENT - RÃ©ponse de haute qualitÃ© !")
                    elif validation['quality_score'] >= 60:
                        print("\nğŸ‘ BON - RÃ©ponse satisfaisante")
                    elif validation['quality_score'] >= 40:
                        print("\nâš ï¸  MOYEN - RÃ©ponse Ã  amÃ©liorer")
                    else:
                        print("\nâŒ FAIBLE - RÃ©ponse nÃ©cessite des amÃ©liorations")
                
                return {
                    "question": question,
                    "response": response,
                    "processing_time": processing_time,
                    "document_count": len(documents),
                    "validation": validation,
                    "success": True
                }
            
            else:
                if verbose:
                    print("âŒ ERREUR: Aucune rÃ©ponse gÃ©nÃ©rÃ©e")
                return {
                    "question": question,
                    "response": None,
                    "processing_time": processing_time,
                    "success": False,
                    "error": "Aucune rÃ©ponse gÃ©nÃ©rÃ©e"
                }
        
        except Exception as e:
            processing_time = time.time() - start_time
            if verbose:
                print(f"âŒ ERREUR ({processing_time:.2f}s): {str(e)}")
                print(f"ğŸ” Type d'erreur: {type(e).__name__}")
                
                # Suggestions selon le type d'erreur
                error_str = str(e).lower()
                if "rate limit" in error_str or "quota" in error_str:
                    print("ğŸ’¡ Suggestion: Limite de taux API atteinte - attendez et rÃ©essayez")
                elif "auth" in error_str or "api_key" in error_str:
                    print("ğŸ’¡ Suggestion: ProblÃ¨me d'authentification - vÃ©rifiez votre clÃ© API")
                elif "connection" in error_str or "network" in error_str:
                    print("ğŸ’¡ Suggestion: ProblÃ¨me de connexion rÃ©seau")
                elif "not found" in error_str:
                    print("ğŸ’¡ Suggestion: VÃ©rifiez que vos documents sont indexÃ©s")
                else:
                    print("ğŸ’¡ Suggestion: VÃ©rifiez la configuration et les logs ci-dessus")
            
            return {
                "question": question,
                "response": None,
                "processing_time": processing_time,
                "success": False,
                "error": str(e)
            }
    
    async def run_all_tests(self):
        """ExÃ©cute tous les tests et gÃ©nÃ¨re un rapport."""
        
        print("ğŸš€ DÃ‰BUT DE LA SUITE COMPLÃˆTE DE TESTS")
        print("=" * 80)
        
        results = []
        total_start_time = time.time()
        
        for i, question in enumerate(self.test_questions, 1):
            print(f"\nğŸ“ TEST {i}/{len(self.test_questions)}")
            result = await self.test_single_question(question, verbose=True)
            results.append(result)
            
            # Pause entre les tests pour Ã©viter les limites de taux
            if i < len(self.test_questions):
                print("\nâ¸ï¸  Pause de 2 secondes...")
                await asyncio.sleep(2)
        
        total_time = time.time() - total_start_time
        
        # GÃ©nÃ©rer le rapport final
        self.generate_report(results, total_time)
        
        return results
    
    def generate_report(self, results, total_time):
        """GÃ©nÃ¨re un rapport de synthÃ¨se des tests."""
        
        print("\n" + "=" * 80)
        print("ğŸ“Š RAPPORT DE SYNTHÃˆSE - RAG ANSD AMÃ‰LIORÃ‰")
        print("=" * 80)
        
        successful_tests = [r for r in results if r['success']]
        failed_tests = [r for r in results if not r['success']]
        
        print(f"âœ… Tests rÃ©ussis: {len(successful_tests)}/{len(results)} ({len(successful_tests)/len(results)*100:.0f}%)")
        print(f"âŒ Tests Ã©chouÃ©s: {len(failed_tests)}/{len(results)}")
        print(f"â±ï¸  Temps total: {total_time:.2f}s")
        print(f"â±ï¸  Temps moyen par test: {total_time/len(results):.2f}s")
        
        if successful_tests:
            # Statistiques de qualitÃ©
            quality_scores = [r['validation']['quality_score'] for r in successful_tests]
            avg_quality = sum(quality_scores) / len(quality_scores)
            max_quality = max(quality_scores)
            min_quality = min(quality_scores)
            
            avg_docs = sum(r['document_count'] for r in successful_tests) / len(successful_tests)
            avg_time = sum(r['processing_time'] for r in successful_tests) / len(successful_tests)
            
            print(f"\nğŸ“ˆ MÃ‰TRIQUES DE QUALITÃ‰:")
            print(f"   ğŸ“Š Score moyen: {avg_quality:.1f}/100")
            print(f"   ğŸ† Meilleur score: {max_quality:.0f}/100")
            print(f"   ğŸ“‰ Score le plus bas: {min_quality:.0f}/100")
            print(f"   ğŸ“š Documents utilisÃ©s en moyenne: {avg_docs:.1f}")
            print(f"   âš¡ Temps de rÃ©ponse moyen: {avg_time:.2f}s")
            
            # Classification des rÃ©sultats
            excellent = len([s for s in quality_scores if s >= 80])
            good = len([s for s in quality_scores if 60 <= s < 80])
            average = len([s for s in quality_scores if 40 <= s < 60])
            poor = len([s for s in quality_scores if s < 40])
            
            print(f"\nğŸ¯ RÃ‰PARTITION DES PERFORMANCES:")
            print(f"   ğŸ‰ Excellent (80-100): {excellent}")
            print(f"   ğŸ‘ Bon (60-79): {good}")
            print(f"   âš ï¸  Moyen (40-59): {average}")
            print(f"   âŒ Faible (<40): {poor}")
        
        # DÃ©tailler les Ã©checs
        if failed_tests:
            print(f"\nâŒ DÃ‰TAIL DES Ã‰CHECS:")
            for i, test in enumerate(failed_tests, 1):
                print(f"   {i}. {test['question']}")
                print(f"      ğŸ’¥ Erreur: {test['error']}")
        
        # Top 3 des meilleures rÃ©ponses
        if successful_tests and len(successful_tests) >= 3:
            best_tests = sorted(successful_tests, key=lambda x: x['validation']['quality_score'], reverse=True)[:3]
            print(f"\nğŸ† TOP 3 DES MEILLEURES RÃ‰PONSES:")
            for i, test in enumerate(best_tests, 1):
                score = test['validation']['quality_score']
                time_taken = test['processing_time']
                print(f"   {i}. {test['question']}")
                print(f"      ğŸ“Š Score: {score}/100 | â±ï¸ {time_taken:.2f}s")
        
        # Recommandations finales
        print(f"\nğŸ’¡ RECOMMANDATIONS:")
        if avg_quality >= 75:
            print("   ğŸ‰ Excellent ! Votre systÃ¨me RAG ANSD fonctionne parfaitement.")
            print("   âœ… PrÃªt pour la production")
        elif avg_quality >= 60:
            print("   ğŸ‘ Bon fonctionnement avec quelques amÃ©liorations possibles.")
            print("   ğŸ”§ Affinez les prompts et la rÃ©cupÃ©ration de documents")
        elif avg_quality >= 40:
            print("   âš ï¸  Performances moyennes - amÃ©liorations nÃ©cessaires.")
            print("   ğŸ”§ VÃ©rifiez la qualitÃ© des documents indexÃ©s")
            print("   ğŸ”§ AmÃ©liorez les prompts systÃ¨me")
        else:
            print("   âŒ Performances faibles - rÃ©vision complÃ¨te recommandÃ©e.")
            print("   ğŸ”§ VÃ©rifiez la configuration et les documents")
            print("   ğŸ”§ Consultez la documentation technique")
        
        print("=" * 80)

async def quick_test():
    """Test rapide avec une question simple."""
    
    tester = ANSDRAGTester()
    result = await tester.test_single_question(
        "Quelle est la population du SÃ©nÃ©gal selon le RGPH ?",
        verbose=True
    )
    return result

async def performance_test():
    """Test de performance avec chronomÃ©trage dÃ©taillÃ©."""
    
    print("âš¡ TEST DE PERFORMANCE")
    print("=" * 50)
    
    tester = ANSDRAGTester()
    questions = [
        "Quelle est la population totale du SÃ©nÃ©gal ?",
        "Quel est le taux de pauvretÃ© ?",
        "Quels sont les indicateurs dÃ©mographiques ?"
    ]
    
    times = []
    scores = []
    
    for i, question in enumerate(questions, 1):
        print(f"\nğŸ“ Test {i}/{len(questions)}: {question[:50]}...")
        start = time.time()
        result = await tester.test_single_question(question, verbose=False)
        end = time.time()
        
        processing_time = end - start
        times.append(processing_time)
        
        if result['success']:
            score = result['validation']['quality_score']
            scores.append(score)
            status = "âœ…"
            print(f"{status} RÃ©ussi en {processing_time:.2f}s (QualitÃ©: {score}/100)")
        else:
            status = "âŒ"
            print(f"{status} Ã‰chouÃ© en {processing_time:.2f}s - {result.get('error', 'Erreur inconnue')}")
    
    print(f"\nğŸ“Š RÃ‰SULTATS DE PERFORMANCE:")
    print(f"   â±ï¸  Temps moyen: {sum(times)/len(times):.2f}s")
    print(f"   â±ï¸  Temps total: {sum(times):.2f}s")
    print(f"   ğŸš€ Test le plus rapide: {min(times):.2f}s")
    print(f"   ğŸŒ Test le plus lent: {max(times):.2f}s")
    
    if scores:
        print(f"   ğŸ“Š QualitÃ© moyenne: {sum(scores)/len(scores):.1f}/100")

async def test_specific_features():
    """Teste des fonctionnalitÃ©s spÃ©cifiques du systÃ¨me amÃ©liorÃ©."""
    
    print("ğŸ”§ TEST DES FONCTIONNALITÃ‰S SPÃ‰CIFIQUES")
    print("=" * 60)
    
    # Test 1: VÃ©rifier que les amÃ©liorations sont actives
    print("\n1ï¸âƒ£ VÃ©rification des amÃ©liorations ANSD:")
    
    # VÃ©rifier les imports des nouvelles fonctions
    try:
        from simple_rag.graph import preprocess_query_enhanced, format_docs_with_rich_metadata
        print("   âœ… Fonctions amÃ©liorÃ©es importÃ©es")
        
        # Test du prÃ©traitement
        test_query = "population SÃ©nÃ©gal"
        enhanced_query = preprocess_query_enhanced(test_query)
        print(f"   âœ… PrÃ©traitement: '{test_query}' â†’ '{enhanced_query}'")
        
    except ImportError:
        print("   âŒ Fonctions amÃ©liorÃ©es non disponibles")
        print("   ğŸ’¡ VÃ©rifiez que vous avez bien remplacÃ© src/simple_rag/graph.py")
    
    # Test 2: Configuration amÃ©liorÃ©e
    print("\n2ï¸âƒ£ Test de la configuration amÃ©liorÃ©e:")
    try:
        from simple_rag.configuration import RagConfiguration
        config = RagConfiguration()
        
        enhanced_features = [
            'enable_query_preprocessing',
            'enable_document_scoring',
            'ansd_survey_weights'
        ]
        
        for feature in enhanced_features:
            if hasattr(config, feature):
                print(f"   âœ… {feature}: configurÃ©")
            else:
                print(f"   âŒ {feature}: manquant")
                
    except Exception as e:
        print(f"   âŒ Erreur configuration: {e}")
    
    # Test 3: Test avec question spÃ©cifique ANSD
    print("\n3ï¸âƒ£ Test avec terminologie ANSD spÃ©cifique:")
    tester = ANSDRAGTester()
    
    ansd_questions = [
        "RÃ©sultats du RGPH-5 sur la population",
        "DonnÃ©es de l'EDS sur la santÃ© maternelle",
        "Indicateurs ESPS sur la pauvretÃ©"
    ]
    
    for question in ansd_questions:
        print(f"\n   ğŸ” Test: {question}")
        result = await tester.test_single_question(question, verbose=False)
        
        if result['success']:
            score = result['validation']['quality_score']
            response_preview = result['response'][:100] + "..." if len(result['response']) > 100 else result['response']
            print(f"   âœ… Score: {score}/100")
            print(f"   ğŸ“ AperÃ§u: {response_preview}")
        else:
            print(f"   âŒ Ã‰chec: {result.get('error', 'Erreur inconnue')}")
    
    print("=" * 60)

async def main():
    """Fonction principale pour exÃ©cuter les tests."""
    
    print("ğŸ‡¸ğŸ‡³ TESTEUR RAG ANSD - SYSTÃˆME AMÃ‰LIORÃ‰ v2")
    print("=" * 80)
    print("ğŸ“„ Configuration .env chargÃ©e avec python-dotenv")
    print("ğŸ”§ Version avec nouvelles fonctionnalitÃ©s activÃ©es")
    print("=" * 80)
    
    # Menu des options
    print("\nOptions de test disponibles:")
    print("1. Test rapide (1 question)")
    print("2. Test de performance (3 questions, focus vitesse)")
    print("3. Test de fonctionnalitÃ©s spÃ©cifiques")
    print("4. Suite complÃ¨te de tests (5 questions)")
    print("5. Quitter")
    
    try:
        choice = input("\nChoisissez une option (1-5): ").strip()
        
        if choice == "1":
            await quick_test()
        elif choice == "2":
            await performance_test()
        elif choice == "3":
            await test_specific_features()
        elif choice == "4":
            tester = ANSDRAGTester()
            await tester.run_all_tests()
        elif choice == "5":
            print("ğŸ‘‹ Au revoir !")
            return
        else:
            print("âŒ Option invalide. ExÃ©cution du test rapide par dÃ©faut.")
            await quick_test()
    
    except KeyboardInterrupt:
        print("\nâ¹ï¸  Tests interrompus par l'utilisateur.")
    except Exception as e:
        print(f"\nâŒ Erreur lors de l'exÃ©cution des tests: {e}")
        print("ğŸ”§ Suggestions de dÃ©pannage:")
        print("   â€¢ VÃ©rifiez vos clÃ©s API dans le fichier .env")
        print("   â€¢ VÃ©rifiez que tous les modules sont installÃ©s")
        print("   â€¢ Consultez les logs d'erreur ci-dessus")

if __name__ == "__main__":
    # ExÃ©cuter les tests
    asyncio.run(main())