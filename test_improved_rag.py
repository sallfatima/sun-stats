#!/usr/bin/env python3
"""
Script de test pour valider les am√©liorations du syst√®me RAG ANSD.
Version 2 avec chargement automatique du fichier .env
"""

# =============================================================================
# CHARGEMENT DU FICHIER .env (PREMI√àRE CHOSE √Ä FAIRE)
# =============================================================================
from dotenv import load_dotenv
load_dotenv()
print("‚úÖ Fichier .env charg√©")

import asyncio
import time
import sys
import os

# Ajouter le r√©pertoire src au path Python
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

# V√©rification des cl√©s API apr√®s chargement .env
def verify_api_keys():
    """V√©rifie que les cl√©s API essentielles sont charg√©es."""
    print("\nüîç V√©rification des cl√©s API:")
    
    api_keys = {
        'OPENAI_API_KEY': ('OpenAI', True),  # (description, required)
        'ANTHROPIC_API_KEY': ('Anthropic Claude', False),
        'PINECONE_API_KEY': ('Pinecone Vector DB', False),
        'LANGSMITH_API_KEY': ('LangSmith Tracing', False)
    }
    
    missing_required = []
    
    for key, (description, required) in api_keys.items():
        value = os.getenv(key)
        if value and value.strip():
            # Masquer les cl√©s sensibles
            masked_value = value[:10] + '...' if len(value) > 10 else '***'
            print(f"   ‚úÖ {description}: {masked_value}")
        else:
            if required:
                print(f"   ‚ùå {description}: MANQUANTE (REQUIS)")
                missing_required.append(key)
            else:
                print(f"   ‚ö™ {description}: Non configur√©e (optionnel)")
    
    if missing_required:
        print(f"\nüö® Erreur: Cl√©s requises manquantes: {', '.join(missing_required)}")
        print("üí° V√©rifiez votre fichier .env")
        return False
    
    print("‚úÖ Toutes les cl√©s requises sont configur√©es")
    return True

# V√©rifier les cl√©s API
if not verify_api_keys():
    print("\n‚èπÔ∏è  Arr√™t du script - Configuration API incompl√®te")
    sys.exit(1)

# Imports du syst√®me RAG (apr√®s v√©rification des cl√©s)
try:
    from langchain_core.messages import HumanMessage
    print("‚úÖ langchain_core import√©")
    
    # Import direct du module sans passer par __init__.py pour √©viter les imports circulaires
    try:
        from simple_rag.graph import graph
        from simple_rag.configuration import RagConfiguration
        print("‚úÖ Modules RAG import√©s")
    except ImportError as e:
        print(f"‚ö†Ô∏è  Import direct √©chou√©: {e}")
        # Essayer import alternatif
        sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src', 'simple_rag'))
        from graph import graph
        from configuration import RagConfiguration
        print("‚úÖ Modules RAG import√©s (m√©thode alternative)")

except ImportError as e:
    print(f"‚ùå Erreur d'import critique: {e}")
    print("üí° V√©rifiez que toutes les d√©pendances sont install√©es:")
    print("   pip install langchain-core langgraph python-dotenv")
    sys.exit(1)

# Fonction de validation des r√©ponses (basique, sans d√©pendances externes)
def validate_ansd_response(response: str) -> dict:
    """Valide qu'une r√©ponse √©tendue contient les √©l√©ments requis pour l'ANSD."""
    
    validation = {
        'has_numerical_data': False,
        'has_source_citation': False,
        'has_year_reference': False,
        'has_ansd_terminology': False,
        'has_structure': False,
        'is_comprehensive': False,
        'has_external_ansd_knowledge': False,
        'quality_score': 0.0,
        'suggestions': []
    }
    
    response_lower = response.lower()
    
    # V√©rifier la pr√©sence de donn√©es num√©riques
    import re
    if re.search(r'\d+(?:[.,]\d+)?(?:\s*%|\s*millions?|\s*milliards?|\s*habitants?)', response):
        validation['has_numerical_data'] = True
        validation['quality_score'] += 15
    else:
        validation['suggestions'].append("Ajouter des donn√©es chiffr√©es pr√©cises")
    
    # V√©rifier les citations de sources ANSD (documents + publications)
    source_indicators = [
        'source :', 'page ', 'document', 'rgph', 'eds', 'esps', 'ehcvm', 'enes', 
        'ansd', 'recensement', 'enqu√™te', 'rapport', 'publication'
    ]
    if any(term in response_lower for term in source_indicators):
        validation['has_source_citation'] = True
        validation['quality_score'] += 15
    else:
        validation['suggestions'].append("Citer les sources ANSD sp√©cifiques")
    
    # V√©rifier les r√©f√©rences temporelles
    if re.search(r'20\d{2}|ann√©e\s+de\s+r√©f√©rence', response):
        validation['has_year_reference'] = True
        validation['quality_score'] += 15
    else:
        validation['suggestions'].append("Pr√©ciser l'ann√©e de r√©f√©rence des donn√©es")
    
    # V√©rifier la terminologie ANSD
    ansd_terms = ['statistique', 'd√©mographique', 's√©n√©gal', 'm√©thodologie', 'indicateur']
    if any(term in response_lower for term in ansd_terms):
        validation['has_ansd_terminology'] = True
        validation['quality_score'] += 10
    else:
        validation['suggestions'].append("Utiliser la terminologie statistique appropri√©e")
    
    # V√©rifier la structure de la r√©ponse
    structure_markers = ['**r√©ponse directe**', '**donn√©es pr√©cises**', '**contexte additionnel**', '-']
    if any(marker in response_lower for marker in structure_markers):
        validation['has_structure'] = True
        validation['quality_score'] += 15
    else:
        validation['suggestions'].append("Am√©liorer la structure de la r√©ponse")
    
    # V√©rifier le caract√®re complet/d√©velopp√©
    if len(response) > 500:  # R√©ponse d√©velopp√©e
        validation['is_comprehensive'] = True
        validation['quality_score'] += 15
    else:
        validation['suggestions'].append("D√©velopper davantage la r√©ponse")
    
    # V√©rifier l'utilisation de connaissances ANSD externes
    external_indicators = [
        'selon les publications ansd', 'd\'apr√®s les rapports ansd', 
        'site officiel ansd', 'www.ansd.sn', 'publications officielles'
    ]
    if any(indicator in response_lower for indicator in external_indicators):
        validation['has_external_ansd_knowledge'] = True
        validation['quality_score'] += 15
    else:
        # Pas de p√©nalit√© si pas de connaissances externes, mais bonus si pr√©sentes
        pass
    
    return validation

class ANSDRAGTester:
    """Classe pour tester le syst√®me RAG am√©lior√© de l'ANSD."""
    
    def __init__(self):
        try:
            self.config = RagConfiguration()
            print("‚úÖ Configuration RAG initialis√©e")
        except Exception as e:
            print(f"‚ö†Ô∏è  Erreur configuration RAG: {e}")
            # Configuration par d√©faut minimale
            self.config = type('Config', (), {
                'model': 'openai/gpt-4o',
                'retrieval_k': 15,
                'enable_debug_logs': True
            })()
            print("‚úÖ Configuration par d√©faut utilis√©e")
        
        self.test_questions = [
            "Quelle est la population du S√©n√©gal selon le dernier RGPH ?",
            "Quel est le taux de pauvret√© au S√©n√©gal ?",
            "Qu'est-ce que le recensement de la population ?",
            "Quels sont les indicateurs de sant√© maternelle au S√©n√©gal ?",
            "Comment √©volue le taux d'alphab√©tisation au S√©n√©gal ?"
        ]
    
    async def test_single_question(self, question: str, verbose: bool = True):
        """Teste une question unique et retourne les r√©sultats."""
        
        if verbose:
            print(f"\nüîç TEST: {question}")
            print("=" * 80)
        
        start_time = time.time()
        
        try:
            # Pr√©parer l'√©tat initial
            initial_state = {
                "messages": [HumanMessage(content=question)]
            }
            
            # Configuration pour le graphe
            if hasattr(self.config, '__dict__'):
                config_dict = {"configurable": self.config.__dict__}
            else:
                config_dict = {"configurable": self.config}
            
            # Ex√©cuter le graphe RAG
            result = await graph.ainvoke(initial_state, config=config_dict)
            
            processing_time = time.time() - start_time
            
            # Extraire la r√©ponse
            if result and "messages" in result and result["messages"]:
                response = result["messages"][-1].content
                documents = result.get("documents", [])
                
                if verbose:
                    print(f"‚úÖ R√âPONSE G√âN√âR√âE ({processing_time:.2f}s):")
                    print("-" * 60)
                    print(response)
                    print("-" * 60)
                    print(f"üìö Documents utilis√©s: {len(documents)}")
                    
                    # Validation de la qualit√©
                    validation = validate_ansd_response(response)
                    print(f"\nüìä SCORE DE QUALIT√â: {validation['quality_score']}/100")
                    
                    # D√©tail des v√©rifications
                    checks = {
                        "Donn√©es num√©riques": validation['has_numerical_data'],
                        "Sources ANSD cit√©es": validation['has_source_citation'],
                        "Ann√©e de r√©f√©rence": validation['has_year_reference'],
                        "Terminologie ANSD": validation['has_ansd_terminology'],
                        "Structure claire": validation['has_structure']
                    }
                    
                    print("üîç V√©rifications d√©taill√©es:")
                    for check, passed in checks.items():
                        status = "‚úÖ" if passed else "‚ùå"
                        print(f"   {status} {check}")
                    
                    if validation['suggestions']:
                        print("\nüí° SUGGESTIONS D'AM√âLIORATION:")
                        for suggestion in validation['suggestions']:
                            print(f"   ‚Ä¢ {suggestion}")
                    
                    # √âvaluation globale
                    if validation['quality_score'] >= 80:
                        print("\nüéâ EXCELLENT - R√©ponse de haute qualit√© !")
                    elif validation['quality_score'] >= 60:
                        print("\nüëç BON - R√©ponse satisfaisante")
                    elif validation['quality_score'] >= 40:
                        print("\n‚ö†Ô∏è  MOYEN - R√©ponse √† am√©liorer")
                    else:
                        print("\n‚ùå FAIBLE - R√©ponse n√©cessite des am√©liorations")
                
                return {
                    "question": question,
                    "response": response,
                    "processing_time": processing_time,
                    "document_count": len(documents),
                    "validation": validation,
                    "success": True
                }
            
            else:
                if verbose:
                    print("‚ùå ERREUR: Aucune r√©ponse g√©n√©r√©e")
                return {
                    "question": question,
                    "response": None,
                    "processing_time": processing_time,
                    "success": False,
                    "error": "Aucune r√©ponse g√©n√©r√©e"
                }
        
        except Exception as e:
            processing_time = time.time() - start_time
            if verbose:
                print(f"‚ùå ERREUR ({processing_time:.2f}s): {str(e)}")
                print(f"üîç Type d'erreur: {type(e).__name__}")
                
                # Suggestions selon le type d'erreur
                error_str = str(e).lower()
                if "rate limit" in error_str or "quota" in error_str:
                    print("üí° Suggestion: Limite de taux API atteinte - attendez et r√©essayez")
                elif "auth" in error_str or "api_key" in error_str:
                    print("üí° Suggestion: Probl√®me d'authentification - v√©rifiez votre cl√© API")
                elif "connection" in error_str or "network" in error_str:
                    print("üí° Suggestion: Probl√®me de connexion r√©seau")
                elif "not found" in error_str:
                    print("üí° Suggestion: V√©rifiez que vos documents sont index√©s")
                else:
                    print("üí° Suggestion: V√©rifiez la configuration et les logs ci-dessus")
            
            return {
                "question": question,
                "response": None,
                "processing_time": processing_time,
                "success": False,
                "error": str(e)
            }
    
    async def run_all_tests(self):
        """Ex√©cute tous les tests et g√©n√®re un rapport."""
        
        print("üöÄ D√âBUT DE LA SUITE COMPL√àTE DE TESTS")
        print("=" * 80)
        
        results = []
        total_start_time = time.time()
        
        for i, question in enumerate(self.test_questions, 1):
            print(f"\nüìù TEST {i}/{len(self.test_questions)}")
            result = await self.test_single_question(question, verbose=True)
            results.append(result)
            
            # Pause entre les tests pour √©viter les limites de taux
            if i < len(self.test_questions):
                print("\n‚è∏Ô∏è  Pause de 2 secondes...")
                await asyncio.sleep(2)
        
        total_time = time.time() - total_start_time
        
        # G√©n√©rer le rapport final
        self.generate_report(results, total_time)
        
        return results
    
    def generate_report(self, results, total_time):
        """G√©n√®re un rapport de synth√®se des tests."""
        
        print("\n" + "=" * 80)
        print("üìä RAPPORT DE SYNTH√àSE - RAG ANSD AM√âLIOR√â")
        print("=" * 80)
        
        successful_tests = [r for r in results if r['success']]
        failed_tests = [r for r in results if not r['success']]
        
        print(f"‚úÖ Tests r√©ussis: {len(successful_tests)}/{len(results)} ({len(successful_tests)/len(results)*100:.0f}%)")
        print(f"‚ùå Tests √©chou√©s: {len(failed_tests)}/{len(results)}")
        print(f"‚è±Ô∏è  Temps total: {total_time:.2f}s")
        print(f"‚è±Ô∏è  Temps moyen par test: {total_time/len(results):.2f}s")
        
        if successful_tests:
            # Statistiques de qualit√©
            quality_scores = [r['validation']['quality_score'] for r in successful_tests]
            avg_quality = sum(quality_scores) / len(quality_scores)
            max_quality = max(quality_scores)
            min_quality = min(quality_scores)
            
            avg_docs = sum(r['document_count'] for r in successful_tests) / len(successful_tests)
            avg_time = sum(r['processing_time'] for r in successful_tests) / len(successful_tests)
            
            print(f"\nüìà M√âTRIQUES DE QUALIT√â:")
            print(f"   üìä Score moyen: {avg_quality:.1f}/100")
            print(f"   üèÜ Meilleur score: {max_quality:.0f}/100")
            print(f"   üìâ Score le plus bas: {min_quality:.0f}/100")
            print(f"   üìö Documents utilis√©s en moyenne: {avg_docs:.1f}")
            print(f"   ‚ö° Temps de r√©ponse moyen: {avg_time:.2f}s")
            
            # Classification des r√©sultats
            excellent = len([s for s in quality_scores if s >= 80])
            good = len([s for s in quality_scores if 60 <= s < 80])
            average = len([s for s in quality_scores if 40 <= s < 60])
            poor = len([s for s in quality_scores if s < 40])
            
            print(f"\nüéØ R√âPARTITION DES PERFORMANCES:")
            print(f"   üéâ Excellent (80-100): {excellent}")
            print(f"   üëç Bon (60-79): {good}")
            print(f"   ‚ö†Ô∏è  Moyen (40-59): {average}")
            print(f"   ‚ùå Faible (<40): {poor}")
        
        # D√©tailler les √©checs
        if failed_tests:
            print(f"\n‚ùå D√âTAIL DES √âCHECS:")
            for i, test in enumerate(failed_tests, 1):
                print(f"   {i}. {test['question']}")
                print(f"      üí• Erreur: {test['error']}")
        
        # Top 3 des meilleures r√©ponses
        if successful_tests and len(successful_tests) >= 3:
            best_tests = sorted(successful_tests, key=lambda x: x['validation']['quality_score'], reverse=True)[:3]
            print(f"\nüèÜ TOP 3 DES MEILLEURES R√âPONSES:")
            for i, test in enumerate(best_tests, 1):
                score = test['validation']['quality_score']
                time_taken = test['processing_time']
                print(f"   {i}. {test['question']}")
                print(f"      üìä Score: {score}/100 | ‚è±Ô∏è {time_taken:.2f}s")
        
        # Recommandations finales
        print(f"\nüí° RECOMMANDATIONS:")
        if avg_quality >= 75:
            print("   üéâ Excellent ! Votre syst√®me RAG ANSD fonctionne parfaitement.")
            print("   ‚úÖ Pr√™t pour la production")
        elif avg_quality >= 60:
            print("   üëç Bon fonctionnement avec quelques am√©liorations possibles.")
            print("   üîß Affinez les prompts et la r√©cup√©ration de documents")
        elif avg_quality >= 40:
            print("   ‚ö†Ô∏è  Performances moyennes - am√©liorations n√©cessaires.")
            print("   üîß V√©rifiez la qualit√© des documents index√©s")
            print("   üîß Am√©liorez les prompts syst√®me")
        else:
            print("   ‚ùå Performances faibles - r√©vision compl√®te recommand√©e.")
            print("   üîß V√©rifiez la configuration et les documents")
            print("   üîß Consultez la documentation technique")
        
        print("=" * 80)

async def quick_test():
    """Test rapide avec une question simple."""
    
    tester = ANSDRAGTester()
    result = await tester.test_single_question(
        "Quelle est la population du S√©n√©gal selon le RGPH ?",
        verbose=True
    )
    return result

async def performance_test():
    """Test de performance avec chronom√©trage d√©taill√©."""
    
    print("‚ö° TEST DE PERFORMANCE")
    print("=" * 50)
    
    tester = ANSDRAGTester()
    questions = [
        "Quelle est la population totale du S√©n√©gal ?",
        "Quel est le taux de pauvret√© ?",
        "Quels sont les indicateurs d√©mographiques ?"
    ]
    
    times = []
    scores = []
    
    for i, question in enumerate(questions, 1):
        print(f"\nüìù Test {i}/{len(questions)}: {question[:50]}...")
        start = time.time()
        result = await tester.test_single_question(question, verbose=False)
        end = time.time()
        
        processing_time = end - start
        times.append(processing_time)
        
        if result['success']:
            score = result['validation']['quality_score']
            scores.append(score)
            status = "‚úÖ"
            print(f"{status} R√©ussi en {processing_time:.2f}s (Qualit√©: {score}/100)")
        else:
            status = "‚ùå"
            print(f"{status} √âchou√© en {processing_time:.2f}s - {result.get('error', 'Erreur inconnue')}")
    
    print(f"\nüìä R√âSULTATS DE PERFORMANCE:")
    print(f"   ‚è±Ô∏è  Temps moyen: {sum(times)/len(times):.2f}s")
    print(f"   ‚è±Ô∏è  Temps total: {sum(times):.2f}s")
    print(f"   üöÄ Test le plus rapide: {min(times):.2f}s")
    print(f"   üêå Test le plus lent: {max(times):.2f}s")
    
    if scores:
        print(f"   üìä Qualit√© moyenne: {sum(scores)/len(scores):.1f}/100")

async def test_specific_features():
    """Teste des fonctionnalit√©s sp√©cifiques du syst√®me am√©lior√©."""
    
    print("üîß TEST DES FONCTIONNALIT√âS SP√âCIFIQUES")
    print("=" * 60)
    
    # Test 1: V√©rifier que les am√©liorations sont actives
    print("\n1Ô∏è‚É£ V√©rification des am√©liorations ANSD:")
    
    # V√©rifier les imports des nouvelles fonctions
    try:
        from simple_rag.graph import preprocess_query_enhanced, format_docs_with_rich_metadata
        print("   ‚úÖ Fonctions am√©lior√©es import√©es")
        
        # Test du pr√©traitement
        test_query = "population S√©n√©gal"
        enhanced_query = preprocess_query_enhanced(test_query)
        print(f"   ‚úÖ Pr√©traitement: '{test_query}' ‚Üí '{enhanced_query}'")
        
    except ImportError:
        print("   ‚ùå Fonctions am√©lior√©es non disponibles")
        print("   üí° V√©rifiez que vous avez bien remplac√© src/simple_rag/graph.py")
    
    # Test 2: Configuration am√©lior√©e
    print("\n2Ô∏è‚É£ Test de la configuration am√©lior√©e:")
    try:
        from simple_rag.configuration import RagConfiguration
        config = RagConfiguration()
        
        enhanced_features = [
            'enable_query_preprocessing',
            'enable_document_scoring',
            'ansd_survey_weights'
        ]
        
        for feature in enhanced_features:
            if hasattr(config, feature):
                print(f"   ‚úÖ {feature}: configur√©")
            else:
                print(f"   ‚ùå {feature}: manquant")
                
    except Exception as e:
        print(f"   ‚ùå Erreur configuration: {e}")
    
    # Test 3: Test avec question sp√©cifique ANSD
    print("\n3Ô∏è‚É£ Test avec terminologie ANSD sp√©cifique:")
    tester = ANSDRAGTester()
    
    ansd_questions = [
        "R√©sultats du RGPH-5 sur la population",
        "Donn√©es de l'EDS sur la sant√© maternelle",
        "Indicateurs ESPS sur la pauvret√©"
    ]
    
    for question in ansd_questions:
        print(f"\n   üîç Test: {question}")
        result = await tester.test_single_question(question, verbose=False)
        
        if result['success']:
            score = result['validation']['quality_score']
            response_preview = result['response'][:100] + "..." if len(result['response']) > 100 else result['response']
            print(f"   ‚úÖ Score: {score}/100")
            print(f"   üìù Aper√ßu: {response_preview}")
        else:
            print(f"   ‚ùå √âchec: {result.get('error', 'Erreur inconnue')}")
    
    print("=" * 60)

async def main():
    """Fonction principale pour ex√©cuter les tests."""
    
    print("üá∏üá≥ TESTEUR RAG ANSD - SYST√àME AM√âLIOR√â v2")
    print("=" * 80)
    print("üìÑ Configuration .env charg√©e avec python-dotenv")
    print("üîß Version avec nouvelles fonctionnalit√©s activ√©es")
    print("=" * 80)
    
    # Menu des options
    print("\nOptions de test disponibles:")
    print("1. Test rapide (1 question)")
    print("2. Test de performance (3 questions, focus vitesse)")
    print("3. Test de fonctionnalit√©s sp√©cifiques")
    print("4. Suite compl√®te de tests (5 questions)")
    print("5. Quitter")
    
    try:
        choice = input("\nChoisissez une option (1-5): ").strip()
        
        if choice == "1":
            await quick_test()
        elif choice == "2":
            await performance_test()
        elif choice == "3":
            await test_specific_features()
        elif choice == "4":
            tester = ANSDRAGTester()
            await tester.run_all_tests()
        elif choice == "5":
            print("üëã Au revoir !")
            return
        else:
            print("‚ùå Option invalide. Ex√©cution du test rapide par d√©faut.")
            await quick_test()
    
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  Tests interrompus par l'utilisateur.")
    except Exception as e:
        print(f"\n‚ùå Erreur lors de l'ex√©cution des tests: {e}")
        print("üîß Suggestions de d√©pannage:")
        print("   ‚Ä¢ V√©rifiez vos cl√©s API dans le fichier .env")
        print("   ‚Ä¢ V√©rifiez que tous les modules sont install√©s")
        print("   ‚Ä¢ Consultez les logs d'erreur ci-dessus")

if __name__ == "__main__":
    # Ex√©cuter les tests
    asyncio.run(main())