import chainlit as cl
import sys
import os
from pathlib import Path
import asyncio
from typing import List, Dict, Any, Tuple

# Ajouter le r√©pertoire src au path Python
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

# =============================================================================
# IMPORTS S√âCURIS√âS AVEC GESTION D'ERREURS
# =============================================================================

def safe_import_rag_graph():
    """Import s√©curis√© de RAGGraph avec gestion des erreurs LangGraph"""
    try:
        print("üîÑ Tentative d'import simple_rag.graph...")
        from simple_rag.graph import graph as simple_rag_graph
        print("‚úÖ Import LangGraph r√©ussi")
        return simple_rag_graph, "langgraph"
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur import LangGraph: {e}")
        try:
            print("üîÑ Tentative d'import RAGGraph classe...")
            from simple_rag.graph import RAGGraph
            print("‚úÖ Import classe RAGGraph r√©ussi")
            return RAGGraph(), "class"
        except Exception as e2:
            print(f"‚ùå Erreur import classe RAGGraph: {e2}")
            print("üîÑ Utilisation de DirectRAG comme fallback...")
            # Cr√©er DirectRAG comme solution de secours
            return create_direct_rag_fallback(), "class"

def create_direct_rag_fallback():
    """Cr√©e une instance DirectRAG comme solution de secours"""
    
    class DirectRAGFallback:
        def __init__(self):
            self.retriever = None
            self.llm = None
            self.setup()
        
        def setup(self):
            try:
                print("üîß Configuration DirectRAG Fallback...")
                
                # Configuration LLM
                from langchain_openai import ChatOpenAI
                self.llm = ChatOpenAI(
                    model="gpt-4o-mini",
                    temperature=0.1
                )
                print("‚úÖ LLM configur√©")
                
                # Configuration Pinecone
                self.setup_pinecone_retriever()
                
            except Exception as e:
                print(f"‚ùå Erreur setup DirectRAG Fallback: {e}")
        
        def setup_pinecone_retriever(self):
            try:
                print("üîå Configuration Pinecone...")
                from langchain_pinecone import PineconeVectorStore
                from langchain_openai import OpenAIEmbeddings
                
                embeddings = OpenAIEmbeddings()
                vectorstore = PineconeVectorStore.from_existing_index(
                    index_name=os.getenv('PINECONE_INDEX', 'index-ansd'),
                    embedding=embeddings
                )
                self.retriever = vectorstore.as_retriever(search_kwargs={"k": 15})
                print("‚úÖ Pinecone configur√©")
                
            except Exception as e:
                print(f"‚ùå Erreur configuration Pinecone: {e}")
                self.retriever = None
        
        def is_ready(self):
            return self.llm is not None and self.retriever is not None
        
        async def ask(self, question: str, chat_history: list) -> tuple:
            try:
                if not self.is_ready():
                    return "‚ùå Syst√®me non configur√©", []
                
                # R√©cup√©ration documents
                print("üîç R√©cup√©ration documents Pinecone...")
                loop = asyncio.get_event_loop()
                
                def get_docs():
                    return self.retriever.get_relevant_documents(question)
                
                documents = await loop.run_in_executor(None, get_docs)
                print(f"üìÑ {len(documents)} documents r√©cup√©r√©s")
                
                # Contexte
                context = "\n\n".join([
                    f"Document {i+1}:\n{doc.page_content[:500]}"
                    for i, doc in enumerate(documents[:8])
                ]) if documents else "Aucun document trouv√©."
                
                # Historique
                history_text = "\n\n".join([
                    f"Q: {q[:150]}\nR: {r[:200]}"
                    for q, r in chat_history[-2:]
                ]) if chat_history else ""
                
                # Prompt
                prompt = f"""Tu es un expert statisticien de l'ANSD du S√©n√©gal.

DOCUMENTS ANSD :
{context}

HISTORIQUE :
{history_text}

QUESTION : {question}

R√©ponds uniquement bas√© sur les documents ANSD fournis. Cite tes sources pr√©cis√©ment.

R√âPONSE :"""
                
                # G√©n√©ration
                from langchain_core.messages import HumanMessage
                response = await self.llm.ainvoke([HumanMessage(content=prompt)])
                
                return response.content, documents
                
            except Exception as e:
                return f"‚ùå Erreur DirectRAG: {str(e)}", []
    
    return DirectRAGFallback()

def safe_import_retrieval_graph():
    """Import s√©curis√© du retrieval graph"""
    try:
        from retrieval_graph.graph import graph as retrieval_graph
        return retrieval_graph, "langgraph"
    except ImportError as e:
        print(f"‚ùå Retrieval graph non disponible: {e}")
        return None, None

def safe_import_self_rag():
    """Import s√©curis√© du self RAG graph"""
    try:
        from self_rag.graph import graph as self_rag_graph
        return self_rag_graph, "langgraph"
    except ImportError as e:
        print(f"‚ùå Self RAG non disponible: {e}")
        return None, None

# Configuration des graphiques disponibles
AVAILABLE_GRAPHS = {}

# Initialisation des graphiques
simple_rag, simple_type = safe_import_rag_graph()
if simple_rag:
    AVAILABLE_GRAPHS["simple_rag"] = {
        "name": "Simple RAG",
        "description": "RAG avec Pinecone + support visuel",
        "instance": simple_rag,
        "type": simple_type
    }

retrieval_graph, retrieval_type = safe_import_retrieval_graph()
if retrieval_graph:
    AVAILABLE_GRAPHS["retrieval_graph"] = {
        "name": "Retrieval Graph", 
        "description": "RAG am√©lior√© avec meilleure r√©cup√©ration",
        "instance": retrieval_graph,
        "type": retrieval_type
    }

self_rag, self_type = safe_import_self_rag()
if self_rag:
    AVAILABLE_GRAPHS["self_rag"] = {
        "name": "Self RAG",
        "description": "RAG avec auto-√©valuation et correction",
        "instance": self_rag,
        "type": self_type
    }

print(f"üìä Graphiques disponibles: {list(AVAILABLE_GRAPHS.keys())}")

# =============================================================================
# FONCTIONS DE SUPPORT VISUEL
# =============================================================================

async def detect_and_display_visual_content(documents: List[Any], user_question: str) -> Tuple[List[Any], bool]:
    """
    D√©tecte et affiche automatiquement les √©l√©ments visuels pertinents
    """
    if not documents:
        return documents, False
    
    print(f"üîç Analyse de {len(documents)} documents pour contenu visuel...")
    
    # S√©parer documents textuels et visuels
    text_docs = []
    visual_elements = []
    
    for doc in documents:
        if hasattr(doc, 'metadata') and doc.metadata:
            doc_type = doc.metadata.get('type', '')
            
            if doc_type in ['visual_chart', 'visual_table']:
                visual_elements.append({
                    'type': doc_type,
                    'content': doc.page_content if hasattr(doc, 'page_content') else str(doc),
                    'metadata': doc.metadata
                })
            else:
                text_docs.append(doc)
        else:
            text_docs.append(doc)
    
    print(f"üìù Documents textuels: {len(text_docs)}")
    print(f"üé® √âl√©ments visuels: {len(visual_elements)}")
    
    # Afficher les √©l√©ments visuels
    has_visual = len(visual_elements) > 0
    
    if has_visual:
        await display_visual_elements(visual_elements, user_question)
    
    return text_docs, has_visual

async def display_visual_elements(visual_elements: List[Dict], user_question: str):
    """
    Affiche les √©l√©ments visuels dans Chainlit
    """
    if not visual_elements:
        return
    
    # Message d'introduction pour les √©l√©ments visuels
    intro_msg = f"üìä **√âl√©ments visuels ANSD trouv√©s :**\n*{user_question}*\n"
    await cl.Message(content=intro_msg).send()
    
    for i, element in enumerate(visual_elements, 1):
        element_type = element['type']
        metadata = element['metadata']
        
        # Extraire les informations importantes
        caption = metadata.get('caption', f'√âl√©ment visuel {i}')
        pdf_name = metadata.get('pdf_name', metadata.get('source_pdf', 'Document ANSD'))
        page = metadata.get('page', metadata.get('page_num', 0))
        
        # Cr√©er le titre de l'√©l√©ment
        if element_type == 'visual_chart':
            title = f"üìä **Graphique {i}** : {caption}"
        elif element_type == 'visual_table':
            title = f"üìã **Tableau {i}** : {caption}"
        else:
            title = f"üìÑ **√âl√©ment {i}** : {caption}"
        
        # Informations sur la source
        source_info = f"*Source : {pdf_name}"
        if page:
            source_info += f", page {page}"
        source_info += "*"
        
        # Affichage g√©n√©rique avec contenu tronqu√©
        await cl.Message(
            content=f"{title}\n{source_info}\n\nüìù **Contenu :**\n{element['content'][:500]}..."
        ).send()

# =============================================================================
# FONCTION D'APPEL RAG CORRIG√âE
# =============================================================================

async def call_graph(graph_instance, user_input: str, chat_history: list, graph_type: str, graph_config: dict):
    """Appelle le graphique appropri√© selon son type avec gestion d'erreurs robuste"""
    
    print(f"üîÑ Appel du graphique {graph_type} (type: {graph_config['type']})")
    
    if graph_config["type"] == "class":
        # Pour les classes DirectRAG ou RAGGraph
        print("üìû Appel m√©thode .ask() sur classe")
        try:
            # V√©rifier si la m√©thode est asynchrone
            import inspect
            if inspect.iscoroutinefunction(graph_instance.ask):
                result = await graph_instance.ask(user_input, chat_history)
            else:
                result = graph_instance.ask(user_input, chat_history)
            print(f"‚úÖ R√©sultat classe re√ßu: {type(result)}")
            return result
        except Exception as e:
            print(f"‚ùå Erreur dans appel classe: {e}")
            import traceback
            traceback.print_exc()
            return f"‚ùå Erreur dans le syst√®me RAG: {str(e)}", []
    
    elif graph_config["type"] == "langgraph":
        # Pour les graphiques LangGraph avec gestion d'erreurs am√©lior√©e
        print("üìû Appel LangGraph .ainvoke()")
        
        try:
            from langchain_core.messages import HumanMessage, AIMessage
            
            # Convertir l'historique en messages
            messages = []
            for user_msg, bot_msg in chat_history:
                messages.append(HumanMessage(content=user_msg))
                messages.append(AIMessage(content=bot_msg))
            
            # Ajouter le message actuel
            messages.append(HumanMessage(content=user_input))
            
            print(f"üìù Messages pr√©par√©s: {len(messages)} messages")
            
            # Pr√©parer l'input
            graph_input = {"messages": messages}
            
            # Configuration pour Pinecone
            config = {
                "configurable": {
                    "model": "openai/gpt-4o-mini",
                    "retrieval_k": 15,
                    "use_pinecone": True,
                    "pinecone_index": os.getenv('PINECONE_INDEX', 'index-ansd')
                }
            }
            
            # Strat√©gies d'appel multiples
            result = None
            
            # Strat√©gie 1: Appel standard
            try:
                print("üîß Tentative d'appel standard...")
                result = await graph_instance.ainvoke(graph_input, config=config)
                print("‚úÖ Appel standard r√©ussi")
                
            except Exception as e1:
                print(f"‚ö†Ô∏è Erreur appel standard: {e1}")
                
                # Strat√©gie 2: Sans certaines configurations
                try:
                    print("üîß Tentative sans configuration compl√®te...")
                    simple_config = {"configurable": {"model": "openai/gpt-4o-mini"}}
                    result = await graph_instance.ainvoke(graph_input, config=simple_config)
                    print("‚úÖ Appel simplifi√© r√©ussi")
                    
                except Exception as e2:
                    print(f"‚ö†Ô∏è Erreur appel simplifi√©: {e2}")
                    
                    # Strat√©gie 3: Sans configuration
                    try:
                        print("üîß Tentative sans configuration...")
                        result = await graph_instance.ainvoke(graph_input)
                        print("‚úÖ Appel sans config r√©ussi")
                        
                    except Exception as e3:
                        print(f"‚ùå Toutes les strat√©gies LangGraph ont √©chou√©")
                        print(f"E1: {e1}")
                        print(f"E2: {e2}")
                        print(f"E3: {e3}")
                        return f"‚ùå Erreur LangGraph: {e1}", []
            
            # Traitement du r√©sultat
            if result is None:
                print("‚ùå Aucun r√©sultat obtenu de LangGraph")
                return "‚ùå Aucune r√©ponse g√©n√©r√©e par le syst√®me", []
            
            print(f"üì¶ R√©sultat LangGraph obtenu: {type(result)}")
            
            # Extraire la r√©ponse et les documents
            answer = "Pas de r√©ponse g√©n√©r√©e"
            sources = []
            
            if isinstance(result, dict):
                # Extraire la r√©ponse des messages
                if "messages" in result and result["messages"]:
                    last_message = result["messages"][-1]
                    if hasattr(last_message, 'content'):
                        answer = last_message.content
                    else:
                        answer = str(last_message)
                    print(f"üìù R√©ponse extraite: {len(answer)} caract√®res")
                
                # Extraire les documents sources
                sources = result.get("documents", [])
                print(f"üìÑ Documents trouv√©s: {len(sources)}")
            else:
                answer = str(result)
                print(f"üìù R√©sultat converti en string: {len(answer)} caract√®res")
            
            return answer, sources
            
        except Exception as e:
            print(f"‚ùå Erreur globale dans LangGraph: {e}")
            import traceback
            traceback.print_exc()
            return f"‚ùå Erreur syst√®me LangGraph: {str(e)}", []
    
    else:
        return f"‚ùå Type de graphique non support√©: {graph_config['type']}", []

# =============================================================================
# CALLBACKS CHAINLIT
# =============================================================================

@cl.on_chat_start
async def on_chat_start():
    """Initialisation du chat"""
    
    print("üöÄ Initialisation du chat Chainlit...")
    
    if not AVAILABLE_GRAPHS:
        await cl.Message(
            content="""‚ùå **Aucun syst√®me disponible**

**Probl√®mes possibles :**
‚Ä¢ Cl√© API OpenAI manquante ou invalide
‚Ä¢ Cl√© API Pinecone manquante ou invalide
‚Ä¢ Index Pinecone introuvable
‚Ä¢ Probl√®me de configuration

**Solutions :**
1. V√©rifiez votre fichier `.env` :
   ```
   OPENAI_API_KEY=sk-...
   PINECONE_API_KEY=...
   PINECONE_INDEX=index-ansd
   ```

2. V√©rifiez que votre index Pinecone existe et est accessible

3. Red√©marrez l'application"""
        ).send()
        return
    
    # Configuration automatique avec un seul syst√®me ou s√©lection
    if len(AVAILABLE_GRAPHS) == 1:
        graph_key = list(AVAILABLE_GRAPHS.keys())[0]
        graph_info = AVAILABLE_GRAPHS[graph_key]
        
        cl.user_session.set("selected_graph", graph_key)
        
        await cl.Message(
            content=f"""üá∏üá≥ **Bienvenue dans TERANGA IA - ANSD**

**Assistant Intelligent pour les Statistiques du S√©n√©gal**

‚úÖ **{graph_info['name']}** activ√© avec Pinecone

üìù *{graph_info['description']}*

üìä **Fonctionnalit√©s :**
‚Ä¢ R√©ponses bas√©es sur les donn√©es officielles ANSD
‚Ä¢ Recherche dans l'index Pinecone `{os.getenv('PINECONE_INDEX', 'index-ansd')}`
‚Ä¢ Affichage automatique des graphiques et tableaux
‚Ä¢ Citations pr√©cises des sources et pages
‚Ä¢ Support des enqu√™tes : RGPH, EDS, ESPS, EHCVM, ENES

**üí° Exemples de questions :**
‚Ä¢ *"Quelle est la population du S√©n√©gal selon le dernier RGPH ?"*
‚Ä¢ *"R√©partition des m√©nages par r√©gion selon la nature du rev√™tement du toit"*
‚Ä¢ *"√âvolution de la population du S√©n√©gal par ann√©e"*
‚Ä¢ *"Taux de pauvret√© par r√©gion administrative"*

**üîß Commandes disponibles :**
‚Ä¢ `/help` - Afficher l'aide compl√®te
‚Ä¢ `/debug` - Informations de diagnostic
‚Ä¢ `/clear` - Effacer l'historique

**üéØ Posez votre question sur les statistiques du S√©n√©gal !**"""
        ).send()
    else:
        # Interface de s√©lection multiple
        actions = [
            cl.Action(
                name=graph_key,
                value=graph_key,
                label=f"{graph_info['name']}: {graph_info['description']}",
                payload={"graph_type": graph_key}
            )
            for graph_key, graph_info in AVAILABLE_GRAPHS.items()
        ]
        
        await cl.Message(
            content="ü§ñ **Bienvenue dans le Chatbot RGPH ‚Äì ANSD**\n\nChoisissez le type de RAG que vous souhaitez utiliser:",
            actions=actions
        ).send()
        
        cl.user_session.set("selected_graph", None)
    
    # Initialiser les variables de session
    cl.user_session.set("chat_history", [])
    print("‚úÖ Session Chainlit initialis√©e")

# Callbacks pour la s√©lection des graphiques
@cl.action_callback("simple_rag")
async def on_simple_rag_selected(action):
    await select_graph("simple_rag")

@cl.action_callback("retrieval_graph") 
async def on_retrieval_graph_selected(action):
    await select_graph("retrieval_graph")

@cl.action_callback("self_rag")
async def on_self_rag_selected(action):
    await select_graph("self_rag")

async def select_graph(graph_type: str):
    """S√©lectionne le graphique choisi"""
    if graph_type not in AVAILABLE_GRAPHS:
        await cl.Message(
            content=f"‚ùå Graphique {graph_type} non disponible"
        ).send()
        return
    
    cl.user_session.set("selected_graph", graph_type)
    graph_info = AVAILABLE_GRAPHS[graph_type]
    
    await cl.Message(
        content=f"""‚úÖ **{graph_info['name']}** s√©lectionn√© !

üìù *{graph_info['description']}*

üéØ Vous pouvez maintenant poser vos questions sur les donn√©es RGPH, EDS, ESPS et autres enqu√™tes nationales.

Les graphiques et tableaux seront affich√©s automatiquement quand ils sont pertinents pour votre question."""
    ).send()

@cl.on_message
async def main(message):
    """Traitement principal des messages avec support visuel automatique"""
    
    try:
        print(f"üì® Message re√ßu: {message.content}")
        
        # Gestion des commandes
        content = message.content.lower().strip()
        
        # Commande de changement de graphique
        if content.startswith("/switch") and len(AVAILABLE_GRAPHS) > 1:
            parts = content.split()
            if len(parts) == 2 and parts[1] in AVAILABLE_GRAPHS:
                await select_graph(parts[1])
            else:
                available = ", ".join(AVAILABLE_GRAPHS.keys())
                await cl.Message(
                    content=f"**Usage:** `/switch [graph_type]`\n\n**Graphiques disponibles:** {available}"
                ).send()
            return
        
        # Commande d'aide
        if content in ["/help", "/aide", "aide", "help"]:
            help_text = f"""üÜò **Aide - Assistant ANSD**

**üìä Types de questions support√©es :**
‚Ä¢ Statistiques d√©mographiques (population, m√©nages, etc.)
‚Ä¢ Donn√©es √©conomiques (PIB, emploi, pauvret√©, etc.)
‚Ä¢ Indicateurs sociaux (√©ducation, sant√©, etc.)
‚Ä¢ R√©partitions g√©ographiques (par r√©gion, milieu, etc.)
‚Ä¢ √âvolutions temporelles et tendances

**üé® Affichage automatique :**
‚Ä¢ Graphiques : Affich√©s automatiquement quand pertinents
‚Ä¢ Tableaux : Format√©s avec donn√©es compl√®tes
‚Ä¢ Sources : Citations pr√©cises avec PDF et page

**üí° Conseils pour de meilleures r√©ponses :**
‚Ä¢ Soyez sp√©cifique : "population urbaine Dakar 2023"
‚Ä¢ Mentionnez le type de donn√©es : "taux de pauvret√©", "r√©partition"
‚Ä¢ Pr√©cisez la zone : "par r√©gion", "milieu rural/urbain"

**üîß Commandes disponibles :**
‚Ä¢ `/help` ou `/aide` : Afficher cette aide
‚Ä¢ `/debug` : Informations techniques
‚Ä¢ `/clear` : Effacer l'historique"""
            
            if len(AVAILABLE_GRAPHS) > 1:
                help_text += f"\n‚Ä¢ `/switch [type]` : Changer de RAG\n\n**Graphiques disponibles :**\n"
                for key, info in AVAILABLE_GRAPHS.items():
                    help_text += f"‚Ä¢ `{key}` - {info['name']}: {info['description']}\n"
            
            help_text += f"\n\n**üîå Configuration Pinecone :**\nIndex actuel : `{os.getenv('PINECONE_INDEX', 'index-ansd')}`"
            
            await cl.Message(content=help_text).send()
            return
        
        # Commande de debug
        if content == "/debug":
            debug_info = f"""üîß **Informations de Debug**

**üèóÔ∏è Configuration :**
‚Ä¢ Graphiques disponibles : {list(AVAILABLE_GRAPHS.keys())}
‚Ä¢ Graphique actuel : {cl.user_session.get("selected_graph", "Non s√©lectionn√©")}
‚Ä¢ Support visuel : ‚úÖ Activ√©

**üîå Configuration Pinecone :**
‚Ä¢ PINECONE_API_KEY : {'‚úÖ Configur√©e' if os.getenv('PINECONE_API_KEY') else '‚ùå Manquante'}
‚Ä¢ PINECONE_INDEX : {os.getenv('PINECONE_INDEX', 'index-ansd')}
‚Ä¢ Index configur√© : {'‚úÖ Oui' if os.getenv('PINECONE_INDEX') else '‚ö†Ô∏è Valeur par d√©faut'}

**üîë Autres API :**
‚Ä¢ OpenAI : {'‚úÖ Configur√©e' if os.getenv('OPENAI_API_KEY') else '‚ùå Manquante'}

**üìÅ Dossiers optionnels :**
‚Ä¢ Images : {Path('images').exists() and len(list(Path('images').glob('*.png')))} fichiers
‚Ä¢ Tableaux : {Path('tables').exists() and len(list(Path('tables').glob('*.csv')))} fichiers

**üíæ Session :**
‚Ä¢ Historique : {len(cl.user_session.get('chat_history', []))} √©changes"""
            
            await cl.Message(content=debug_info).send()
            return
        
        # Commande de nettoyage
        if content == "/clear":
            cl.user_session.set("chat_history", [])
            await cl.Message(content="üßπ **Historique effac√©**\n\nL'historique de conversation a √©t√© remis √† z√©ro.").send()
            return
        
        # V√©rifier qu'un graphique a √©t√© s√©lectionn√©
        selected_graph = cl.user_session.get("selected_graph")
        
        if not selected_graph:
            await cl.Message(
                content="‚ö†Ô∏è **Veuillez d'abord s√©lectionner un type de RAG**\n\nUtilisez les boutons ci-dessus ou tapez `/help` pour voir les commandes disponibles."
            ).send()
            return
        
        if selected_graph not in AVAILABLE_GRAPHS:
            await cl.Message(
                content=f"‚ùå Graphique {selected_graph} non disponible\n\nUtilisez `/switch [type]` pour changer de graphique."
            ).send()
            return
        
        # R√©cup√©rer l'historique
        chat_history = cl.user_session.get("chat_history", [])
        
        # Extraire le texte du message
        user_input = message.content
        
        print(f"‚ùì Question: {user_input}")
        print(f"üìö Historique: {len(chat_history)} √©changes")
        
        # Limiter l'historique envoy√© (garder les 10 derniers √©changes)
        short_history = chat_history[-10:]
        
        # Afficher un indicateur de traitement
        graph_info = AVAILABLE_GRAPHS[selected_graph]
        processing_msg = await cl.Message(
            content=f"üîç **Recherche dans Pinecone ({os.getenv('PINECONE_INDEX', 'index-ansd')})...**\n\n*Analyse en cours avec {graph_info['name']} et d√©tection automatique du contenu visuel*"
        ).send()
        
        # Appeler le graphique appropri√©
        print(f"üîÑ Appel du graphique {selected_graph}...")
        answer, documents = await call_graph(
            graph_info["instance"], 
            user_input, 
            short_history, 
            selected_graph,
            graph_info
        )
        
        print(f"‚úÖ R√©ponse re√ßue: {len(answer) if answer else 0} chars, {len(documents) if documents else 0} docs")
        
        # Supprimer le message de traitement
        await processing_msg.remove()
        
        # D√©tecter et afficher automatiquement le contenu visuel
        has_visual = False
        if documents:
            print(f"üìÑ {len(documents)} documents r√©cup√©r√©s")
            try:
                text_docs, has_visual = await detect_and_display_visual_content(documents, user_input)
                print(f"üé® Contenu visuel d√©tect√©: {has_visual}")
                
                if has_visual:
                    # Ajouter une note √† la r√©ponse
                    answer += "\n\n*üìä Les √©l√©ments visuels correspondants sont affich√©s ci-dessus.*"
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur d√©tection visuel: {e}")
        
        # Mettre √† jour l'historique
        chat_history.append((user_input, answer))
        
        # Limiter la taille de l'historique
        if len(chat_history) > 20:
            chat_history = chat_history[-20:]
        
        cl.user_session.set("chat_history", chat_history)
        
        # Envoyer la r√©ponse
        await cl.Message(content=answer).send()
        
        print(f"‚úÖ R√©ponse envoy√©e. Historique: {len(chat_history)} √©changes")
        
    except Exception as e:
        print(f"‚ùå Erreur dans main: {e}")
        import traceback
        traceback.print_exc()
        
        await cl.Message(
            content=f"""‚ùå **Erreur technique**

Une erreur s'est produite lors du traitement de votre question.

**D√©tails:** {str(e)}

**Solutions sugg√©r√©es:**
‚Ä¢ V√©rifiez vos cl√©s API (OpenAI, Pinecone)
‚Ä¢ V√©rifiez que votre index Pinecone est accessible
‚Ä¢ Red√©marrez l'application avec `chainlit run chainlit_app.py`
‚Ä¢ Tapez `/debug` pour plus d'informations
‚Ä¢ Reformulez votre question

Veuillez r√©essayer."""
        ).send()

# =============================================================================
# D√âMARRAGE
# =============================================================================

if __name__ == "__main__":
    print("üöÄ Lancement de TERANGA IA - ANSD")
    print("üìä Syst√®me RAG avec Pinecone et support visuel")
    print("üîó Interface Chainlit pr√™te")
    print(f"üìà Graphiques disponibles: {list(AVAILABLE_GRAPHS.keys())}")
    print(f"üîå Index Pinecone: {os.getenv('PINECONE_INDEX', 'index-ansd')}")
    
    if not AVAILABLE_GRAPHS:
        print("‚ö†Ô∏è ATTENTION: Aucun graphique disponible!")
        print("V√©rifiez vos imports et votre configuration Pinecone.")
    
    # L'application sera lanc√©e avec: chainlit run chainlit_app.py