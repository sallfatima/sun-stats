import chainlit as cl
import sys
import os
from pathlib import Path
import asyncio
from typing import List, Dict, Any, Tuple

# Ajouter le rÃ©pertoire src au path Python
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

# =============================================================================
# IMPORTS SÃ‰CURISÃ‰S AVEC GESTION D'ERREURS
# =============================================================================

def safe_import_rag_graph():
    """Import sÃ©curisÃ© de RAGGraph avec gestion des erreurs LangGraph"""
    try:
        print("ğŸ”„ Tentative d'import simple_rag.graph...")
        from simple_rag.graph import graph as simple_rag_graph
        print("âœ… Import LangGraph rÃ©ussi")
        return simple_rag_graph, "langgraph"
    except Exception as e:
        print(f"âš ï¸ Erreur import LangGraph: {e}")
        try:
            print("ğŸ”„ Tentative d'import RAGGraph classe...")
            from simple_rag.graph import RAGGraph
            print("âœ… Import classe RAGGraph rÃ©ussi")
            return RAGGraph(), "class"
        except Exception as e2:
            print(f"âŒ Erreur import classe RAGGraph: {e2}")
            print("ğŸ”„ Utilisation de DirectRAG comme fallback...")
            # CrÃ©er DirectRAG comme solution de secours
            return create_direct_rag_fallback(), "class"

def create_direct_rag_fallback():
    """CrÃ©e une instance DirectRAG comme solution de secours"""
    
    class DirectRAGFallback:
        def __init__(self):
            self.retriever = None
            self.llm = None
            self.setup()
        
        def setup(self):
            try:
                print("ğŸ”§ Configuration DirectRAG Fallback...")
                
                # Configuration LLM
                from langchain_openai import ChatOpenAI
                self.llm = ChatOpenAI(
                    model="gpt-4o-mini",
                    temperature=0.1
                )
                print("âœ… LLM configurÃ©")
                
                # Configuration Pinecone
                self.setup_pinecone_retriever()
                
            except Exception as e:
                print(f"âŒ Erreur setup DirectRAG Fallback: {e}")
        
        def setup_pinecone_retriever(self):
            try:
                print("ğŸ”Œ Configuration Pinecone...")
                from langchain_pinecone import PineconeVectorStore
                from langchain_openai import OpenAIEmbeddings
                
                embeddings = OpenAIEmbeddings()
                vectorstore = PineconeVectorStore.from_existing_index(
                    index_name=os.getenv('PINECONE_INDEX', 'index-ansd'),
                    embedding=embeddings
                )
                self.retriever = vectorstore.as_retriever(search_kwargs={"k": 15})
                print("âœ… Pinecone configurÃ©")
                
            except Exception as e:
                print(f"âŒ Erreur configuration Pinecone: {e}")
                self.retriever = None
        
        def is_ready(self):
            return self.llm is not None and self.retriever is not None
        
        async def ask(self, question: str, chat_history: list) -> tuple:
            try:
                if not self.is_ready():
                    return "âŒ SystÃ¨me non configurÃ©", []
                
                # RÃ©cupÃ©ration documents
                print("ğŸ” RÃ©cupÃ©ration documents Pinecone...")
                loop = asyncio.get_event_loop()
                
                def get_docs():
                    return self.retriever.get_relevant_documents(question)
                
                documents = await loop.run_in_executor(None, get_docs)
                print(f"ğŸ“„ {len(documents)} documents rÃ©cupÃ©rÃ©s")
                
                # Contexte
                context = "\n\n".join([
                    f"Document {i+1}:\n{doc.page_content[:500]}"
                    for i, doc in enumerate(documents[:8])
                ]) if documents else "Aucun document trouvÃ©."
                
                # Historique
                history_text = "\n\n".join([
                    f"Q: {q[:150]}\nR: {r[:200]}"
                    for q, r in chat_history[-2:]
                ]) if chat_history else ""
                
                # Prompt
                prompt = f"""Tu es un expert statisticien de l'ANSD du SÃ©nÃ©gal.

DOCUMENTS ANSD :
{context}

HISTORIQUE :
{history_text}

QUESTION : {question}

RÃ©ponds uniquement basÃ© sur les documents ANSD fournis. Cite tes sources prÃ©cisÃ©ment.

RÃ‰PONSE :"""
                
                # GÃ©nÃ©ration
                from langchain_core.messages import HumanMessage
                response = await self.llm.ainvoke([HumanMessage(content=prompt)])
                
                return response.content, documents
                
            except Exception as e:
                return f"âŒ Erreur DirectRAG: {str(e)}", []
    
    return DirectRAGFallback()

def safe_import_retrieval_graph():
    """Import sÃ©curisÃ© du retrieval graph"""
    try:
        from retrieval_graph.graph import graph as retrieval_graph
        return retrieval_graph, "langgraph"
    except ImportError as e:
        print(f"âŒ Retrieval graph non disponible: {e}")
        return None, None

def safe_import_self_rag():
    """Import sÃ©curisÃ© du self RAG graph"""
    try:
        from self_rag.graph import graph as self_rag_graph
        return self_rag_graph, "langgraph"
    except ImportError as e:
        print(f"âŒ Self RAG non disponible: {e}")
        return None, None

# Configuration des graphiques disponibles
AVAILABLE_GRAPHS = {}

# Initialisation des graphiques
simple_rag, simple_type = safe_import_rag_graph()
if simple_rag:
    AVAILABLE_GRAPHS["simple_rag"] = {
        "name": "Simple RAG",
        "description": "RAG avec Pinecone + support visuel",
        "instance": simple_rag,
        "type": simple_type
    }

retrieval_graph, retrieval_type = safe_import_retrieval_graph()
if retrieval_graph:
    AVAILABLE_GRAPHS["retrieval_graph"] = {
        "name": "Retrieval Graph", 
        "description": "RAG amÃ©liorÃ© avec meilleure rÃ©cupÃ©ration",
        "instance": retrieval_graph,
        "type": retrieval_type
    }

self_rag, self_type = safe_import_self_rag()
if self_rag:
    AVAILABLE_GRAPHS["self_rag"] = {
        "name": "Self RAG",
        "description": "RAG avec auto-Ã©valuation et correction",
        "instance": self_rag,
        "type": self_type
    }

print(f"ğŸ“Š Graphiques disponibles: {list(AVAILABLE_GRAPHS.keys())}")

# =============================================================================
# FONCTIONS DE SUPPORT VISUEL
# =============================================================================

async def detect_and_display_visual_content(documents: List[Any], user_question: str) -> Tuple[List[Any], bool]:
    """
    DÃ©tecte et affiche automatiquement les Ã©lÃ©ments visuels pertinents
    """
    if not documents:
        return documents, False
    
    print(f"ğŸ” Analyse de {len(documents)} documents pour contenu visuel...")
    
    # SÃ©parer documents textuels et visuels
    text_docs = []
    visual_elements = []
    
    for doc in documents:
        if hasattr(doc, 'metadata') and doc.metadata:
            doc_type = doc.metadata.get('type', '')
            
            if doc_type in ['visual_chart', 'visual_table']:
                visual_elements.append({
                    'type': doc_type,
                    'content': doc.page_content if hasattr(doc, 'page_content') else str(doc),
                    'metadata': doc.metadata
                })
            else:
                text_docs.append(doc)
        else:
            text_docs.append(doc)
    
    print(f"ğŸ“ Documents textuels: {len(text_docs)}")
    print(f"ğŸ¨ Ã‰lÃ©ments visuels: {len(visual_elements)}")
    
    # Afficher les Ã©lÃ©ments visuels
    has_visual = len(visual_elements) > 0
    
    if has_visual:
        await display_visual_elements(visual_elements, user_question)
    
    return text_docs, has_visual

async def display_visual_elements(visual_elements: List[Dict], user_question: str):
    """
    Affiche les Ã©lÃ©ments visuels dans Chainlit
    """
    if not visual_elements:
        return
    
    # Message d'introduction pour les Ã©lÃ©ments visuels
    intro_msg = f"ğŸ“Š **Ã‰lÃ©ments visuels ANSD trouvÃ©s :**\n*{user_question}*\n"
    await cl.Message(content=intro_msg).send()
    
    for i, element in enumerate(visual_elements, 1):
        element_type = element['type']
        metadata = element['metadata']
        
        # Extraire les informations importantes
        caption = metadata.get('caption', f'Ã‰lÃ©ment visuel {i}')
        pdf_name = metadata.get('pdf_name', metadata.get('source_pdf', 'Document ANSD'))
        page = metadata.get('page', metadata.get('page_num', 0))
        
        # CrÃ©er le titre de l'Ã©lÃ©ment
        if element_type == 'visual_chart':
            title = f"ğŸ“Š **Graphique {i}** : {caption}"
        elif element_type == 'visual_table':
            title = f"ğŸ“‹ **Tableau {i}** : {caption}"
        else:
            title = f"ğŸ“„ **Ã‰lÃ©ment {i}** : {caption}"
        
        # Informations sur la source
        source_info = f"*Source : {pdf_name}"
        if page:
            source_info += f", page {page}"
        source_info += "*"
        
        # Affichage gÃ©nÃ©rique avec contenu tronquÃ©
        await cl.Message(
            content=f"{title}\n{source_info}\n\nğŸ“ **Contenu :**\n{element['content'][:500]}..."
        ).send()

# =============================================================================
# FONCTION D'APPEL RAG CORRIGÃ‰E
# =============================================================================

async def call_graph(graph_instance, user_input: str, chat_history: list, graph_type: str, graph_config: dict):
    """Appelle le graphique appropriÃ© selon son type avec gestion d'erreurs robuste"""
    
    print(f"ğŸ”„ Appel du graphique {graph_type} (type: {graph_config['type']})")
    
    if graph_config["type"] == "class":
        # Pour les classes DirectRAG ou RAGGraph
        print("ğŸ“ Appel mÃ©thode .ask() sur classe")
        try:
            # VÃ©rifier si la mÃ©thode est asynchrone
            import inspect
            if inspect.iscoroutinefunction(graph_instance.ask):
                result = await graph_instance.ask(user_input, chat_history)
            else:
                result = graph_instance.ask(user_input, chat_history)
            print(f"âœ… RÃ©sultat classe reÃ§u: {type(result)}")
            return result
        except Exception as e:
            print(f"âŒ Erreur dans appel classe: {e}")
            import traceback
            traceback.print_exc()
            return f"âŒ Erreur dans le systÃ¨me RAG: {str(e)}", []
    
    elif graph_config["type"] == "langgraph":
        # Pour les graphiques LangGraph avec gestion d'erreurs amÃ©liorÃ©e
        print("ğŸ“ Appel LangGraph .ainvoke()")
        
        try:
            from langchain_core.messages import HumanMessage, AIMessage
            
            # Convertir l'historique en messages
            messages = []
            for user_msg, bot_msg in chat_history:
                messages.append(HumanMessage(content=user_msg))
                messages.append(AIMessage(content=bot_msg))
            
            # Ajouter le message actuel
            messages.append(HumanMessage(content=user_input))
            
            print(f"ğŸ“ Messages prÃ©parÃ©s: {len(messages)} messages")
            
            # PrÃ©parer l'input
            graph_input = {"messages": messages}
            
            # Configuration pour Pinecone
            config = {
                "configurable": {
                    "model": "openai/gpt-4o-mini",
                    "retrieval_k": 15,
                    "use_pinecone": True,
                    "pinecone_index": os.getenv('PINECONE_INDEX', 'index-ansd')
                }
            }
            
            # StratÃ©gies d'appel multiples
            result = None
            
            # StratÃ©gie 1: Appel standard
            try:
                print("ğŸ”§ Tentative d'appel standard...")
                result = await graph_instance.ainvoke(graph_input, config=config)
                print("âœ… Appel standard rÃ©ussi")
                
            except Exception as e1:
                print(f"âš ï¸ Erreur appel standard: {e1}")
                
                # StratÃ©gie 2: Sans certaines configurations
                try:
                    print("ğŸ”§ Tentative sans configuration complÃ¨te...")
                    simple_config = {"configurable": {"model": "openai/gpt-4o-mini"}}
                    result = await graph_instance.ainvoke(graph_input, config=simple_config)
                    print("âœ… Appel simplifiÃ© rÃ©ussi")
                    
                except Exception as e2:
                    print(f"âš ï¸ Erreur appel simplifiÃ©: {e2}")
                    
                    # StratÃ©gie 3: Sans configuration
                    try:
                        print("ğŸ”§ Tentative sans configuration...")
                        result = await graph_instance.ainvoke(graph_input)
                        print("âœ… Appel sans config rÃ©ussi")
                        
                    except Exception as e3:
                        print(f"âŒ Toutes les stratÃ©gies LangGraph ont Ã©chouÃ©")
                        print(f"E1: {e1}")
                        print(f"E2: {e2}")
                        print(f"E3: {e3}")
                        return f"âŒ Erreur LangGraph: {e1}", []
            
            # Traitement du rÃ©sultat
            if result is None:
                print("âŒ Aucun rÃ©sultat obtenu de LangGraph")
                return "âŒ Aucune rÃ©ponse gÃ©nÃ©rÃ©e par le systÃ¨me", []
            
            print(f"ğŸ“¦ RÃ©sultat LangGraph obtenu: {type(result)}")
            
            # Extraire la rÃ©ponse et les documents
            answer = "Pas de rÃ©ponse gÃ©nÃ©rÃ©e"
            sources = []
            
            if isinstance(result, dict):
                # Extraire la rÃ©ponse des messages
                if "messages" in result and result["messages"]:
                    last_message = result["messages"][-1]
                    if hasattr(last_message, 'content'):
                        answer = last_message.content
                    else:
                        answer = str(last_message)
                    print(f"ğŸ“ RÃ©ponse extraite: {len(answer)} caractÃ¨res")
                
                # Extraire les documents sources
                sources = result.get("documents", [])
                print(f"ğŸ“„ Documents trouvÃ©s: {len(sources)}")
            else:
                answer = str(result)
                print(f"ğŸ“ RÃ©sultat converti en string: {len(answer)} caractÃ¨res")
            
            return answer, sources
            
        except Exception as e:
            print(f"âŒ Erreur globale dans LangGraph: {e}")
            import traceback
            traceback.print_exc()
            return f"âŒ Erreur systÃ¨me LangGraph: {str(e)}", []
    
    else:
        return f"âŒ Type de graphique non supportÃ©: {graph_config['type']}", []

# =============================================================================
# CALLBACKS CHAINLIT
# =============================================================================

@cl.on_chat_start
async def on_chat_start():
    """Initialisation du chat"""
    
    print("ğŸš€ Initialisation du chat Chainlit...")
    
    if not AVAILABLE_GRAPHS:
        await cl.Message(
            content="""âŒ **Aucun systÃ¨me disponible**

**ProblÃ¨mes possibles :**
â€¢ ClÃ© API OpenAI manquante ou invalide
â€¢ ClÃ© API Pinecone manquante ou invalide
â€¢ Index Pinecone introuvable
â€¢ ProblÃ¨me de configuration

**Solutions :**
1. VÃ©rifiez votre fichier `.env` :
   ```
   OPENAI_API_KEY=sk-...
   PINECONE_API_KEY=...
   PINECONE_INDEX=index-ansd
   ```

2. VÃ©rifiez que votre index Pinecone existe et est accessible

3. RedÃ©marrez l'application"""
        ).send()
        return
    
    # Configuration automatique avec un seul systÃ¨me ou sÃ©lection
    if len(AVAILABLE_GRAPHS) == 1:
        graph_key = list(AVAILABLE_GRAPHS.keys())[0]
        graph_info = AVAILABLE_GRAPHS[graph_key]
        
        cl.user_session.set("selected_graph", graph_key)
        
        await cl.Message(
            content=f"""ğŸ‡¸ğŸ‡³ **Bienvenue dans TERANGA IA - ANSD**

**Assistant Intelligent pour les Statistiques du SÃ©nÃ©gal**

âœ… **{graph_info['name']}** activÃ© avec Pinecone

ğŸ“ *{graph_info['description']}*

ğŸ“Š **FonctionnalitÃ©s :**
â€¢ RÃ©ponses basÃ©es sur les donnÃ©es officielles ANSD
â€¢ Recherche dans l'index Pinecone `{os.getenv('PINECONE_INDEX', 'index-ansd')}`
â€¢ Affichage automatique des graphiques et tableaux
â€¢ Citations prÃ©cises des sources et pages
â€¢ Support des enquÃªtes : RGPH, EDS, ESPS, EHCVM, ENES

**ğŸ’¡ Exemples de questions :**
â€¢ *"Quelle est la population du SÃ©nÃ©gal selon le dernier RGPH ?"*
â€¢ *"RÃ©partition des mÃ©nages par rÃ©gion selon la nature du revÃªtement du toit"*
â€¢ *"Ã‰volution de la population du SÃ©nÃ©gal par annÃ©e"*
â€¢ *"Taux de pauvretÃ© par rÃ©gion administrative"*

**ğŸ”§ Commandes disponibles :**
â€¢ `/help` - Afficher l'aide complÃ¨te
â€¢ `/debug` - Informations de diagnostic
â€¢ `/clear` - Effacer l'historique

**ğŸ¯ Posez votre question sur les statistiques du SÃ©nÃ©gal !**"""
        ).send()
    else:
        # Interface de sÃ©lection multiple
        actions = [
            cl.Action(
                name=graph_key,
                value=graph_key,
                label=f"{graph_info['name']}: {graph_info['description']}",
                payload={"graph_type": graph_key}
            )
            for graph_key, graph_info in AVAILABLE_GRAPHS.items()
        ]
        
        await cl.Message(
            content="ğŸ¤– **Bienvenue dans le Chatbot RGPH â€“ ANSD**\n\nChoisissez le type de RAG que vous souhaitez utiliser:",
            actions=actions
        ).send()
        
        cl.user_session.set("selected_graph", None)
    
    # Initialiser les variables de session
    cl.user_session.set("chat_history", [])
    print("âœ… Session Chainlit initialisÃ©e")

# Callbacks pour la sÃ©lection des graphiques
@cl.action_callback("simple_rag")
async def on_simple_rag_selected(action):
    await select_graph("simple_rag")

@cl.action_callback("retrieval_graph") 
async def on_retrieval_graph_selected(action):
    await select_graph("retrieval_graph")

@cl.action_callback("self_rag")
async def on_self_rag_selected(action):
    await select_graph("self_rag")

async def select_graph(graph_type: str):
    """SÃ©lectionne le graphique choisi"""
    if graph_type not in AVAILABLE_GRAPHS:
        await cl.Message(
            content=f"âŒ Graphique {graph_type} non disponible"
        ).send()
        return
    
    cl.user_session.set("selected_graph", graph_type)
    graph_info = AVAILABLE_GRAPHS[graph_type]
    
    await cl.Message(
        content=f"""âœ… **{graph_info['name']}** sÃ©lectionnÃ© !

ğŸ“ *{graph_info['description']}*

ğŸ¯ Vous pouvez maintenant poser vos questions sur les donnÃ©es RGPH, EDS, ESPS et autres enquÃªtes nationales.

Les graphiques et tableaux seront affichÃ©s automatiquement quand ils sont pertinents pour votre question."""
    ).send()

@cl.on_message
async def main(message):
    """Traitement principal des messages avec support visuel automatique"""
    
    try:
        print(f"ğŸ“¨ Message reÃ§u: {message.content}")
        
        # Gestion des commandes
        content = message.content.lower().strip()
        
        # Commande de changement de graphique
        if content.startswith("/switch") and len(AVAILABLE_GRAPHS) > 1:
            parts = content.split()
            if len(parts) == 2 and parts[1] in AVAILABLE_GRAPHS:
                await select_graph(parts[1])
            else:
                available = ", ".join(AVAILABLE_GRAPHS.keys())
                await cl.Message(
                    content=f"**Usage:** `/switch [graph_type]`\n\n**Graphiques disponibles:** {available}"
                ).send()
            return
        
        # Commande d'aide
        if content in ["/help", "/aide", "aide", "help"]:
            help_text = f"""ğŸ†˜ **Aide - Assistant ANSD**

**ğŸ“Š Types de questions supportÃ©es :**
â€¢ Statistiques dÃ©mographiques (population, mÃ©nages, etc.)
â€¢ DonnÃ©es Ã©conomiques (PIB, emploi, pauvretÃ©, etc.)
â€¢ Indicateurs sociaux (Ã©ducation, santÃ©, etc.)
â€¢ RÃ©partitions gÃ©ographiques (par rÃ©gion, milieu, etc.)
â€¢ Ã‰volutions temporelles et tendances

**ğŸ¨ Affichage automatique :**
â€¢ Graphiques : AffichÃ©s automatiquement quand pertinents
â€¢ Tableaux : FormatÃ©s avec donnÃ©es complÃ¨tes
â€¢ Sources : Citations prÃ©cises avec PDF et page

**ğŸ’¡ Conseils pour de meilleures rÃ©ponses :**
â€¢ Soyez spÃ©cifique : "population urbaine Dakar 2023"
â€¢ Mentionnez le type de donnÃ©es : "taux de pauvretÃ©", "rÃ©partition"
â€¢ PrÃ©cisez la zone : "par rÃ©gion", "milieu rural/urbain"

**ğŸ”§ Commandes disponibles :**
â€¢ `/help` ou `/aide` : Afficher cette aide
â€¢ `/debug` : Informations techniques
â€¢ `/clear` : Effacer l'historique"""
            
            if len(AVAILABLE_GRAPHS) > 1:
                help_text += f"\nâ€¢ `/switch [type]` : Changer de RAG\n\n**Graphiques disponibles :**\n"
                for key, info in AVAILABLE_GRAPHS.items():
                    help_text += f"â€¢ `{key}` - {info['name']}: {info['description']}\n"
            
            help_text += f"\n\n**ğŸ”Œ Configuration Pinecone :**\nIndex actuel : `{os.getenv('PINECONE_INDEX', 'index-ansd')}`"
            
            await cl.Message(content=help_text).send()
            return
        
        # Commande de debug
        if content == "/debug":
            debug_info = f"""ğŸ”§ **Informations de Debug**

**ğŸ—ï¸ Configuration :**
â€¢ Graphiques disponibles : {list(AVAILABLE_GRAPHS.keys())}
â€¢ Graphique actuel : {cl.user_session.get("selected_graph", "Non sÃ©lectionnÃ©")}
â€¢ Support visuel : âœ… ActivÃ©

**ğŸ”Œ Configuration Pinecone :**
â€¢ PINECONE_API_KEY : {'âœ… ConfigurÃ©e' if os.getenv('PINECONE_API_KEY') else 'âŒ Manquante'}
â€¢ PINECONE_INDEX : {os.getenv('PINECONE_INDEX', 'index-ansd')}
â€¢ Index configurÃ© : {'âœ… Oui' if os.getenv('PINECONE_INDEX') else 'âš ï¸ Valeur par dÃ©faut'}

**ğŸ”‘ Autres API :**
â€¢ OpenAI : {'âœ… ConfigurÃ©e' if os.getenv('OPENAI_API_KEY') else 'âŒ Manquante'}

**ğŸ“ Dossiers optionnels :**
â€¢ Images : {Path('images').exists() and len(list(Path('images').glob('*.png')))} fichiers
â€¢ Tableaux : {Path('tables').exists() and len(list(Path('tables').glob('*.csv')))} fichiers

**ğŸ’¾ Session :**
â€¢ Historique : {len(cl.user_session.get('chat_history', []))} Ã©changes"""
            
            await cl.Message(content=debug_info).send()
            return
        
        # Commande de nettoyage
        if content == "/clear":
            cl.user_session.set("chat_history", [])
            await cl.Message(content="ğŸ§¹ **Historique effacÃ©**\n\nL'historique de conversation a Ã©tÃ© remis Ã  zÃ©ro.").send()
            return
        
        # VÃ©rifier qu'un graphique a Ã©tÃ© sÃ©lectionnÃ©
        selected_graph = cl.user_session.get("selected_graph")
        
        if not selected_graph:
            await cl.Message(
                content="âš ï¸ **Veuillez d'abord sÃ©lectionner un type de RAG**\n\nUtilisez les boutons ci-dessus ou tapez `/help` pour voir les commandes disponibles."
            ).send()
            return
        
        if selected_graph not in AVAILABLE_GRAPHS:
            await cl.Message(
                content=f"âŒ Graphique {selected_graph} non disponible\n\nUtilisez `/switch [type]` pour changer de graphique."
            ).send()
            return
        
        # RÃ©cupÃ©rer l'historique
        chat_history = cl.user_session.get("chat_history", [])
        
        # Extraire le texte du message
        user_input = message.content
        
        print(f"â“ Question: {user_input}")
        print(f"ğŸ“š Historique: {len(chat_history)} Ã©changes")
        
        # Limiter l'historique envoyÃ© (garder les 10 derniers Ã©changes)
        short_history = chat_history[-10:]
        
        # Afficher un indicateur de traitement
        graph_info = AVAILABLE_GRAPHS[selected_graph]
        processing_msg = await cl.Message(
            content=f"ğŸ” **Recherche dans Pinecone ({os.getenv('PINECONE_INDEX', 'index-ansd')})...**\n\n*Analyse en cours avec {graph_info['name']} et dÃ©tection automatique du contenu visuel*"
        ).send()
        
        # Appeler le graphique appropriÃ©
        print(f"ğŸ”„ Appel du graphique {selected_graph}...")
        answer, documents = await call_graph(
            graph_info["instance"], 
            user_input, 
            short_history, 
            selected_graph,
            graph_info
        )
        
        print(f"âœ… RÃ©ponse reÃ§ue: {len(answer) if answer else 0} chars, {len(documents) if documents else 0} docs")
        
        # Supprimer le message de traitement
        await processing_msg.remove()
        
        # DÃ©tecter et afficher automatiquement le contenu visuel
        has_visual = False
        if documents:
            print(f"ğŸ“„ {len(documents)} documents rÃ©cupÃ©rÃ©s")
            try:
                text_docs, has_visual = await detect_and_display_visual_content(documents, user_input)
                print(f"ğŸ¨ Contenu visuel dÃ©tectÃ©: {has_visual}")
                
                if has_visual:
                    # Ajouter une note Ã  la rÃ©ponse
                    answer += "\n\n*ğŸ“Š Les Ã©lÃ©ments visuels correspondants sont affichÃ©s ci-dessus.*"
            except Exception as e:
                print(f"âš ï¸ Erreur dÃ©tection visuel: {e}")
        
        # Mettre Ã  jour l'historique
        chat_history.append((user_input, answer))
        
        # Limiter la taille de l'historique
        if len(chat_history) > 20:
            chat_history = chat_history[-20:]
        
        cl.user_session.set("chat_history", chat_history)
        
        # Envoyer la rÃ©ponse
        await cl.Message(content=answer).send()
        
        print(f"âœ… RÃ©ponse envoyÃ©e. Historique: {len(chat_history)} Ã©changes")
        
    except Exception as e:
        print(f"âŒ Erreur dans main: {e}")
        import traceback
        traceback.print_exc()
        
        await cl.Message(
            content=f"""âŒ **Erreur technique**

Une erreur s'est produite lors du traitement de votre question.

**DÃ©tails:** {str(e)}

**Solutions suggÃ©rÃ©es:**
â€¢ VÃ©rifiez vos clÃ©s API (OpenAI, Pinecone)
â€¢ VÃ©rifiez que votre index Pinecone est accessible
â€¢ RedÃ©marrez l'application avec `chainlit run chainlit_app.py`
â€¢ Tapez `/debug` pour plus d'informations
â€¢ Reformulez votre question

Veuillez rÃ©essayer."""
        ).send()

# =============================================================================
# DÃ‰MARRAGE
# =============================================================================

if __name__ == "__main__":
    print("ğŸš€ Lancement de TERANGA IA - ANSD")
    print("ğŸ“Š SystÃ¨me RAG avec Pinecone et support visuel")
    print("ğŸ”— Interface Chainlit prÃªte")
    print(f"ğŸ“ˆ Graphiques disponibles: {list(AVAILABLE_GRAPHS.keys())}")
    print(f"ğŸ”Œ Index Pinecone: {os.getenv('PINECONE_INDEX', 'index-ansd')}")
    
    if not AVAILABLE_GRAPHS:
        print("âš ï¸ ATTENTION: Aucun graphique disponible!")
        print("VÃ©rifiez vos imports et votre configuration Pinecone.")
    
    # L'application sera lancÃ©e avec: chainlit run chainlit_app.py