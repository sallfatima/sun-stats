"""Shared utility functions used in the project.

Functions:
    format_docs: Convert documents to an xml-formatted string.
    load_chat_model: Load a chat model from a model name.
"""
import os
from typing import Optional, List, Dict

from langchain.chat_models import init_chat_model
from langchain_core.documents import Document
from langchain_core.language_models import BaseChatModel
from pinecone import Index, Pinecone, ServerlessSpec
from urllib.parse import urljoin 
import re
from typing import List, Dict, Any
from datetime import datetime
import csv
from pathlib import Path
import fitz  # PyMuPDF
import unicodedata

def _format_doc(doc: Document) -> str:
    """Format a single document as XML.

    Args:
        doc (Document): The document to format.

    Returns:
        str: The formatted document as an XML string.
    """
    metadata = doc.metadata or {}
    meta = "".join(f" {k}={v!r}" for k, v in metadata.items())
    if meta:
        meta = f" {meta}"

    return f"<document{meta}>\n{doc.page_content}\n</document>"


def format_docs(docs: Optional[list[Document]]) -> str:
    """Format a list of documents as XML.

    This function takes a list of Document objects and formats them into a single XML string.

    Args:
        docs (Optional[list[Document]]): A list of Document objects to format, or None.

    Returns:
        str: A string containing the formatted documents in XML format.

    Examples:
        >>> docs = [Document(page_content="Hello"), Document(page_content="World")]
        >>> print(format_docs(docs))
        <documents>
        <document>
        Hello
        </document>
        <document>
        World
        </document>
        </documents>

        >>> print(format_docs(None))
        <documents></documents>
    """
    if not docs:
        return "<documents></documents>"
    formatted = "\n".join(_format_doc(doc) for doc in docs)
    return f"""<documents>
{formatted}
</documents>"""

def load_pinecone_index(index_name: str):
    """
    Charge un index Pinecone existant, ou le cr√©e automatiquement s‚Äôil n‚Äôexiste pas.
    """
    pinecone_client = Pinecone(
        api_key=os.environ["PINECONE_API_KEY"],
        environment=os.environ["PINECONE_ENVIRONMENT"]
    )

    indexes = pinecone_client.list_indexes().names()
    print("üîé Index disponibles :", indexes)

    if index_name not in indexes:
        print(f"‚ö†Ô∏è L'index '{index_name}' n'existe pas. Cr√©ation...")
        pinecone_client.create_index(
            name=index_name,
            dimension=1536,
            metric="cosine",
            spec= ServerlessSpec(
                cloud="aws",  # or "gcp"
                region="us-east-1"
            )
        )
        print(f"‚úÖ Index '{index_name}' cr√©√©.")

    return pinecone_client.Index(index_name)

def load_chat_model(fully_specified_name: str) -> BaseChatModel:
    """Load a chat model from a fully specified name.

    Args:
        fully_specified_name (str): String in the format 'provider/model'.
    """
    if "/" in fully_specified_name:
        provider, model = fully_specified_name.split("/", maxsplit=1)
    else:
        provider = ""
        model = fully_specified_name
    return init_chat_model(model, model_provider=provider)
# =============================================================================
# FICHIER 4: src/shared/utils.py
# =============================================================================
# AJOUTEZ CES FONCTIONS √Ä LA FIN DE VOTRE FICHIER EXISTANT (ne pas remplacer le contenu existant)

# =============================================================================
# FONCTIONS UTILITAIRES AM√âLIOR√âES POUR L'ANSD
# =============================================================================

import re
from typing import List, Dict, Any
from datetime import datetime

def detect_ansd_survey_type(text: str) -> List[str]:
    """D√©tecte le type d'enqu√™te ANSD dans un texte."""
    
    text_lower = text.lower()
    surveys = []
    
    # Dictionnaire des enqu√™tes avec leurs variantes
    survey_patterns = {
        "rgph": ["rgph", "recensement", "population", "habitat", "recensement g√©n√©ral"],
        "eds": ["eds", "enqu√™te d√©mographique", "sant√©", "demographic health survey"],
        "esps": ["esps", "pauvret√©", "enqu√™te de suivi", "poverty"],
        "ehcvm": ["ehcvm", "conditions de vie", "m√©nages", "budget", "consommation"],
        "enes": ["enes", "emploi", "ch√¥mage", "activit√© √©conomique"],
        "comptes_nationaux": ["pib", "comptes nationaux", "√©conomie", "croissance"],
    }
    
    for survey, patterns in survey_patterns.items():
        if any(pattern in text_lower for pattern in patterns):
            surveys.append(survey)
    
    return surveys

def extract_numerical_data(text: str) -> List[Dict[str, Any]]:
    """Extrait les donn√©es num√©riques d'un texte avec leur contexte."""
    
    numerical_data = []
    
    # Patterns pour diff√©rents types de donn√©es
    patterns = [
        # Pourcentages
        (r'(\d+(?:[.,]\d+)?)\s*%', 'percentage'),
        # Nombres avec unit√©s mon√©taires
        (r'(\d+(?:[.,]\d+)?)\s*(fcfa|francs?|f\s*cfa)', 'currency'),
        # Populations en millions/milliards
        (r'(\d+(?:[.,]\d+)?)\s*(millions?|milliards?)', 'population'),
        # Ann√©es
        (r'\b(20\d{2})\b', 'year'),
        # Nombres simples avec contexte
        (r'(\d+(?:[.,]\d+)?)', 'number'),
    ]
    
    for pattern, data_type in patterns:
        matches = re.finditer(pattern, text, re.IGNORECASE)
        for match in matches:
            # Extraire le contexte autour du nombre (50 caract√®res avant et apr√®s)
            start_context = max(0, match.start() - 50)
            end_context = min(len(text), match.end() + 50)
            context = text[start_context:end_context].strip()
            
            numerical_data.append({
                'value': match.group(1),
                'type': data_type,
                'context': context,
                'position': match.span()
            })
    
    return numerical_data

def format_ansd_metadata(metadata: Dict[str, Any]) -> str:
    """Formate les m√©tadonn√©es sp√©cifiques aux documents ANSD."""
    
    formatted_parts = []
    
    # Source principale
    if 'source' in metadata:
        formatted_parts.append(f"üìÑ Source: {metadata['source']}")
    
    # Nom du document
    if 'pdf_name' in metadata:
        formatted_parts.append(f"üìã Document: {metadata['pdf_name']}")
    
    # Num√©ro de page
    if 'page_num' in metadata:
        formatted_parts.append(f"üìñ Page: {metadata['page_num']}")
    
    # Date d'indexation
    if 'indexed_at' in metadata:
        date_str = metadata['indexed_at'][:10] if len(metadata['indexed_at']) >= 10 else metadata['indexed_at']
        formatted_parts.append(f"üïê Index√©: {date_str}")
    
    # Type de document
    if 'type' in metadata:
        formatted_parts.append(f"üìä Type: {metadata['type']}")
    
    # Taille du fichier (si disponible)
    if 'file_size' in metadata:
        size_mb = metadata['file_size'] / (1024 * 1024)
        formatted_parts.append(f"üíæ Taille: {size_mb:.1f} MB")
    
    return " | ".join(formatted_parts) if formatted_parts else "üìÑ M√©tadonn√©es non disponibles"

def calculate_document_relevance_score(doc_content: str, query: str, survey_weights: Dict[str, float] = None) -> float:
    """Calcule un score de pertinence pour un document par rapport √† une requ√™te."""
    
    if not doc_content or not query:
        return 0.0
    
    doc_lower = doc_content.lower()
    query_lower = query.lower()
    query_words = set(query_lower.split())
    
    score = 0.0
    
    # Score bas√© sur les mots-cl√©s de la requ√™te
    for word in query_words:
        if len(word) > 3:  # Ignorer les mots tr√®s courts
            word_count = doc_lower.count(word)
            score += word_count * 2
    
    # Bonus pour les termes ANSD sp√©cifiques
    ansd_terms = {
        'rgph': 5,
        'eds': 5,
        'esps': 5,
        'ehcvm': 5,
        'enes': 5,
        'ansd': 3,
        's√©n√©gal': 3,
        'recensement': 4,
        'enqu√™te': 3,
        'statistique': 3
    }
    
    for term, weight in ansd_terms.items():
        if term in doc_lower:
            score += weight
    
    # Bonus pour les donn√©es num√©riques
    numerical_patterns = [
        r'\d+[.,]\d+\s*%',  # Pourcentages
        r'\d+\s*(millions?|milliards?)',  # Grandes populations
        r'\d{4}',  # Ann√©es
    ]
    
    for pattern in numerical_patterns:
        matches = len(re.findall(pattern, doc_lower))
        score += matches * 2
    
    # Appliquer les poids des enqu√™tes si fournis
    if survey_weights:
        surveys = detect_ansd_survey_type(doc_content)
        for survey in surveys:
            if survey in survey_weights:
                score *= survey_weights[survey]
    
    return score

def clean_ansd_text(text: str) -> str:
    """Nettoie le texte des documents ANSD pour am√©liorer la lisibilit√©."""
    
    if not text:
        return ""
    
    # Supprimer les caract√®res de contr√¥le
    text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x9f]', '', text)
    
    # Normaliser les espaces
    text = re.sub(r'\s+', ' ', text)
    
    # Corriger les probl√®mes d'encodage courants
    replacements = {
        '√É ': '√† ',
        '√É¬©': '√©',
        '√É¬®': '√®',
        '√É¬ß': '√ß',
        '√É¬¥': '√¥',
        '√É¬¢': '√¢',
        '√É¬Æ': '√Æ',
        '√É¬ª': '√ª',
        '√É¬π': '√π',
    }
    
    for wrong, correct in replacements.items():
        text = text.replace(wrong, correct)
    
    return text.strip()

def extract_ansd_indicators(text: str) -> List[Dict[str, str]]:
    """Extrait les indicateurs statistiques sp√©cifiques √† l'ANSD."""
    
    indicators = []
    
    # Patterns pour diff√©rents types d'indicateurs
    indicator_patterns = [
        # Taux d√©mographiques
        (r'taux\s+(?:de\s+)?(?:natalit√©|mortalit√©|f√©condit√©|croissance)\s*:\s*([^.\n]+)', 'd√©mographique'),
        # Indicateurs √©conomiques
        (r'(?:pib|produit\s+int√©rieur\s+brut)\s*:\s*([^.\n]+)', '√©conomique'),
        # Indicateurs de pauvret√©
        (r'(?:taux\s+de\s+pauvret√©|incidence\s+de\s+la\s+pauvret√©)\s*:\s*([^.\n]+)', 'pauvret√©'),
        # Indicateurs d'√©ducation
        (r'taux\s+(?:d\s*\'?\s*alphab√©tisation|de\s+scolarisation)\s*:\s*([^.\n]+)', '√©ducation'),
        # Indicateurs de sant√©
        (r'(?:esp√©rance\s+de\s+vie|mortalit√©\s+(?:infantile|maternelle))\s*:\s*([^.\n]+)', 'sant√©'),
    ]
    
    for pattern, category in indicator_patterns:
        matches = re.finditer(pattern, text, re.IGNORECASE)
        for match in matches:
            indicators.append({
                'category': category,
                'indicator': match.group(0),
                'value': match.group(1).strip(),
                'position': match.span()
            })
    
    return indicators

def validate_ansd_response(response: str) -> Dict[str, Any]:
    """Valide qu'une r√©ponse contient les √©l√©ments requis pour l'ANSD."""
    
    validation = {
        'has_numerical_data': False,
        'has_source_citation': False,
        'has_year_reference': False,
        'has_ansd_terminology': False,
        'quality_score': 0.0,
        'suggestions': []
    }
    
    response_lower = response.lower()
    
    # V√©rifier la pr√©sence de donn√©es num√©riques
    if re.search(r'\d+(?:[.,]\d+)?(?:\s*%|\s*millions?|\s*milliards?)', response):
        validation['has_numerical_data'] = True
        validation['quality_score'] += 25
    else:
        validation['suggestions'].append("Ajouter des donn√©es chiffr√©es pr√©cises")
    
    # V√©rifier les citations de sources
    if any(term in response_lower for term in ['source:', 'rgph', 'eds', 'esps', 'ehcvm', 'enes', 'ansd']):
        validation['has_source_citation'] = True
        validation['quality_score'] += 25
    else:
        validation['suggestions'].append("Citer les sources ANSD sp√©cifiques")
    
    # V√©rifier les r√©f√©rences temporelles
    if re.search(r'20\d{2}|ann√©e\s+de\s+r√©f√©rence', response):
        validation['has_year_reference'] = True
        validation['quality_score'] += 25
    else:
        validation['suggestions'].append("Pr√©ciser l'ann√©e de r√©f√©rence des donn√©es")
    
    # V√©rifier la terminologie ANSD
    ansd_terms = ['enqu√™te', 'recensement', 'indicateur', 'statistique', 'd√©mographique', 's√©n√©gal']
    if any(term in response_lower for term in ansd_terms):
        validation['has_ansd_terminology'] = True
        validation['quality_score'] += 25
    else:
        validation['suggestions'].append("Utiliser la terminologie statistique appropri√©e")
    
    return validation

# =============================================================================
# FONCTIONS DE LOGGING SP√âCIALIS√âES
# =============================================================================

def log_ansd_retrieval(query: str, documents: List, processing_time: float = None):
    """Log sp√©cialis√© pour le processus de r√©cup√©ration ANSD."""
    
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    print(f"\n{'='*60}")
    print(f"üïê ANSD RETRIEVAL LOG - {timestamp}")
    print(f"{'='*60}")
    print(f"üìù Requ√™te: {query}")
    print(f"üìö Documents trouv√©s: {len(documents)}")
    
    if processing_time:
        print(f"‚è±Ô∏è  Temps de traitement: {processing_time:.2f}s")
    
    # Analyser les types d'enqu√™tes trouv√©es
    survey_counts = {}
    for doc in documents:
        surveys = detect_ansd_survey_type(doc.page_content)
        for survey in surveys:
            survey_counts[survey] = survey_counts.get(survey, 0) + 1
    
    if survey_counts:
        print(f"üìä Types d'enqu√™tes identifi√©es:")
        for survey, count in survey_counts.items():
            print(f"   ‚Ä¢ {survey.upper()}: {count} document(s)")
    
    print(f"{'='*60}\n")

def log_ansd_generation(response: str, validation_result: Dict = None):
    """Log sp√©cialis√© pour le processus de g√©n√©ration ANSD."""
    
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    print(f"\n{'='*60}")
    print(f"ü§ñ ANSD GENERATION LOG - {timestamp}")
    print(f"{'='*60}")
    print(f"üìè Longueur de la r√©ponse: {len(response)} caract√®res")
    
    if validation_result:
        print(f"‚≠ê Score de qualit√©: {validation_result['quality_score']}/100")
        if validation_result['suggestions']:
            print(f"üí° Suggestions d'am√©lioration:")
            for suggestion in validation_result['suggestions']:
                print(f"   ‚Ä¢ {suggestion}")
    
    # Extraire les donn√©es num√©riques mentionn√©es
    numerical_data = extract_numerical_data(response)
    if numerical_data:
        print(f"üî¢ Donn√©es num√©riques d√©tect√©es: {len(numerical_data)}")
        for data in numerical_data[:3]:  # Afficher les 3 premi√®res
            print(f"   ‚Ä¢ {data['value']} ({data['type']})")
    
    print(f"{'='*60}\n")



# Chemin fixes √† la racine du projet
IMAGES_DIR = Path("./images")
INDEX_CSV = Path("./charts_index.csv")


def generate_charts_index(pdf_root: Path) -> None:
    """
    Extrait toutes les images (charts) des PDF sous pdf_root,
    les sauvegarde dans IMAGES_DIR
    et reconstruit INDEX_CSV.

    :param pdf_root: Dossier racine contenant les PDF √† analyser.
    """
    IMAGES_DIR.mkdir(parents=True, exist_ok=True)

    with INDEX_CSV.open("w", newline="", encoding="utf-8") as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(["image_id", "pdf_path", "page", "image_path"])

        for pdf_path in pdf_root.rglob("*.pdf"):
            doc = fitz.open(str(pdf_path))
            for page_num in range(len(doc)):
                page = doc[page_num]
                for img_index, img in enumerate(page.get_images(full=True), start=1):
                    xref = img[0]
                    pix = fitz.Pixmap(doc, xref)
                    if pix.n > 4:
                        pix = fitz.Pixmap(fitz.csRGB, pix)

                    image_name = f"{pdf_path.stem}_p{page_num+1}_i{img_index}.png"
                    image_path = IMAGES_DIR / image_name
                    pix.save(str(image_path))
                    pix = None

                    image_id = image_name.replace(".png", "")
                    writer.writerow([
                        image_id,
                        str(pdf_path),
                        page_num + 1,
                        str(image_path)
                    ])

    print(f"[‚úî] Generated {INDEX_CSV} with images from {pdf_root}")



def sanitize_pinecone_id(text: str) -> str:
    """
    Convertit un texte en ID valide pour Pinecone (ASCII uniquement).
    
    R√©sout les erreurs comme:
    - 'visual_table_Tableau_VII-7_Proportion_de_c√©libataires_d√©finitif_p5_t7'
    - 'visual_chart_Graphique_IV-16_Pourcentage_de_la_d√©claration_de_p_p38_i2'
    
    Args:
        text: Texte source pouvant contenir des caract√®res non-ASCII
        
    Returns:
        ID valide pour Pinecone (ASCII uniquement)
        
    Examples:
        >>> sanitize_pinecone_id("visual_table_Tableau_VII-7_Proportion_de_c√©libataires_d√©finitif_p5_t7")
        'visual_table_Tableau_VII_7_Proportion_de_celibataires_definitif_p5_t7'
        
        >>> sanitize_pinecone_id("visual_chart_Graphique_IV-16_Pourcentage_de_la_d√©claration_de_p_p38_i2")
        'visual_chart_Graphique_IV_16_Pourcentage_de_la_declaration_de_p_p38_i2'
    """
    if not text:
        return f"item_{hash('empty') % 1000000}"
    
    # 1. Normalisation Unicode (√© ‚Üí e, √† ‚Üí a, √ß ‚Üí c, etc.)
    text = unicodedata.normalize('NFD', text)
    text = ''.join(c for c in text if unicodedata.category(c) != 'Mn')
    
    # 2. Remplacer tous les caract√®res non-ASCII par underscore
    text = re.sub(r'[^a-zA-Z0-9_-]', '_', text)
    
    # 3. Nettoyer les underscores multiples
    text = re.sub(r'_+', '_', text)
    
    # 4. Supprimer les underscores en d√©but/fin
    text = text.strip('_')
    
    # 5. Limiter la longueur (Pinecone max 512, on prend 80 pour s√©curit√©)
    if len(text) > 80:
        text = text[:80].rstrip('_')
    
    # 6. S'assurer qu'on a quelque chose de valide
    if not text or not text.replace('_', '').replace('-', ''):
        text = f"item_{hash(str(text)) % 1000000}"
    
    return text



def test_sanitize_pinecone_id():
    """Teste la fonction de sanitisation avec des cas r√©els."""
    
    test_cases = [
        # Cas probl√©matiques r√©els de votre syst√®me
        "visual_table_Tableau_VII-7_Proportion_de_c√©libataires_d√©finitif_p5_t7",
        "visual_chart_Graphique_IV-16_Pourcentage_de_la_d√©claration_de_p_p38_i2",
        "visual_chart_Graphique_I-10_√âvolution_du_rapport_de_masculinit√©_p27_i1",
        
        # Autres cas √† tester
        "Chapitre 1- ETAT-STRUCTURE-POPULATION-Rapport-Provisoire-RGPH5_juillet2024_0_p28_i1",
        "Graphique_I-11_Pyramide_des_√¢ges_de_la_population_p29_i1",
        "Chapitre 4 - FECONDITE-NATALITE-Rapport-Provisoire-RGPH5_juillet2024_0_p25_i2",
        
        # Cas limites
        "",
        "___---___",
        "a" * 200,  # Trop long
    ]
    
    print("üß™ TEST DE SANITISATION PINECONE IDS")
    print("=" * 80)
    
    for i, test_id in enumerate(test_cases, 1):
        fixed_id = sanitize_pinecone_id(test_id)
        
        print(f"\nTest {i}:")
        print(f"  Avant  : {test_id}")
        print(f"  Apr√®s  : {fixed_id}")
        print(f"  ASCII  : {fixed_id.isascii()}")
        print(f"  Longueur: {len(fixed_id)}")
        print(f"  Valide : {'‚úÖ' if fixed_id.isascii() and len(fixed_id) <= 80 and fixed_id else '‚ùå'}")


# =============================================================================
# UTILITAIRES POUR DEBUGGING DE L'INDEXATION
# =============================================================================

def validate_pinecone_id(text: str) -> Dict[str, Any]:
    """
    Valide qu'un ID est compatible avec Pinecone.
    
    Args:
        text: ID √† valider
        
    Returns:
        Dictionnaire avec les r√©sultats de validation
    """
    validation = {
        'is_valid': True,
        'is_ascii': text.isascii(),
        'length_ok': len(text) <= 512,
        'not_empty': bool(text.strip()),
        'errors': [],
        'warnings': []
    }
    
    if not validation['is_ascii']:
        validation['is_valid'] = False
        validation['errors'].append(f"Contient des caract√®res non-ASCII: {[c for c in text if not c.isascii()]}")
    
    if not validation['length_ok']:
        validation['is_valid'] = False
        validation['errors'].append(f"Trop long: {len(text)} caract√®res (max 512)")
    
    if not validation['not_empty']:
        validation['is_valid'] = False
        validation['errors'].append("ID vide")
    
    if len(text) > 80:
        validation['warnings'].append(f"ID assez long: {len(text)} caract√®res")
    
    return validation


def diagnose_indexation_errors(error_message: str) -> Dict[str, Any]:
    """
    Diagnostique les erreurs d'indexation courantes.
    
    Args:
        error_message: Message d'erreur re√ßu
        
    Returns:
        Diagnostic avec solutions sugg√©r√©es
    """
    diagnosis = {
        'error_type': 'unknown',
        'cause': 'Non identifi√©e',
        'solution': 'V√©rifier les logs d√©taill√©s',
        'code_fix': None
    }
    
    error_lower = error_message.lower()
    
    # Erreur ASCII Pinecone
    if "vector id must be ascii" in error_lower:
        diagnosis.update({
            'error_type': 'pinecone_ascii',
            'cause': 'ID contient des caract√®res non-ASCII (accents, caract√®res sp√©ciaux)',
            'solution': 'Utiliser sanitize_pinecone_id() pour nettoyer les IDs',
            'code_fix': '''
# Dans src/index_graph/graph.py, remplacer:
doc_id = f"visual_chart_{row.get('image_id', idx)}"

# Par:
from shared.utils import sanitize_pinecone_id
raw_id = f"visual_chart_{row.get('image_id', idx)}"
doc_id = sanitize_pinecone_id(raw_id)
'''
        })
    
    # Erreur de limite de taux API
    elif "rate limit" in error_lower or "429" in error_lower:
        diagnosis.update({
            'error_type': 'api_rate_limit',
            'cause': 'Trop d\'appels API simultan√©s vers OpenAI',
            'solution': 'R√©duire visual_batch_size et ajouter des d√©lais',
            'code_fix': '''
# Dans la configuration, r√©duire:
"visual_batch_size": 2  # Au lieu de 5

# Ajouter des d√©lais entre batches:
await asyncio.sleep(2)  # 2 secondes
'''
        })
    
    # Erreur Pinecone de dimension
    elif "dimension" in error_lower:
        diagnosis.update({
            'error_type': 'pinecone_dimension',
            'cause': 'Dimension des vecteurs incorrecte',
            'solution': 'V√©rifier la dimension de l\'embedding model (1536 pour text-embedding-3-small)',
        })
    
    return diagnosis


# =============================================================================
# FONCTIONS DE MONITORING POUR L'INDEXATION
# =============================================================================

def log_indexation_progress(current: int, total: int, item_type: str = "√©l√©ment") -> None:
    """
    Affiche le progr√®s de l'indexation de mani√®re format√©e.
    
    Args:
        current: Nombre d'√©l√©ments trait√©s
        total: Nombre total d'√©l√©ments
        item_type: Type d'√©l√©ment (graphique, tableau, etc.)
    """
    percentage = (current / total) * 100 if total > 0 else 0
    bar_length = 30
    filled_length = int(bar_length * current // total) if total > 0 else 0
    bar = '‚ñà' * filled_length + '‚ñë' * (bar_length - filled_length)
    
    print(f"\rüîÑ Indexation {item_type}: [{bar}] {current}/{total} ({percentage:.1f}%)", end='', flush=True)
    
    if current == total:
        print()  # Nouvelle ligne √† la fin


def create_indexation_report(stats: Dict[str, Any]) -> str:
    """
    Cr√©e un rapport d√©taill√© de l'indexation.
    
    Args:
        stats: Statistiques d'indexation
        
    Returns:
        Rapport format√©
    """
    report_lines = [
        "üìä RAPPORT D'INDEXATION ANSD",
        "=" * 50,
        ""
    ]
    
    # Statistiques g√©n√©rales
    if 'total_text_chunks' in stats:
        report_lines.append(f"üìù Chunks de texte index√©s: {stats['total_text_chunks']:,}")
    
    # Statistiques visuelles
    visual_stats = stats.get('visual_indexing_stats', {})
    if visual_stats:
        report_lines.extend([
            f"üìä Graphiques index√©s: {visual_stats.get('charts_indexed', 0):,}",
            f"üìã Tableaux index√©s: {visual_stats.get('tables_indexed', 0):,}",
            f"‚ùå Graphiques √©chou√©s: {visual_stats.get('charts_failed', 0):,}",
            f"‚ùå Tableaux √©chou√©s: {visual_stats.get('tables_failed', 0):,}",
        ])
    
    # Total
    total_visual = visual_stats.get('charts_indexed', 0) + visual_stats.get('tables_indexed', 0)
    total_content = stats.get('total_text_chunks', 0) + total_visual
    
    report_lines.extend([
        "",
        f"üéØ TOTAL INDEX√â: {total_content:,} √©l√©ments",
        f"   ‚îú‚îÄ Textuel: {stats.get('total_text_chunks', 0):,}",
        f"   ‚îî‚îÄ Visuel: {total_visual:,}",
        ""
    ])
    
    # Fichiers trait√©s
    if 'processed_files' in stats:
        report_lines.append(f"‚úÖ PDFs trait√©s: {len(stats['processed_files'])}")
    
    if 'failed_files' in stats and stats['failed_files']:
        report_lines.append(f"‚ùå PDFs √©chou√©s: {len(stats['failed_files'])}")
    
    # Recommandations
    report_lines.extend([
        "",
        "üí° RECOMMANDATIONS:",
    ])
    
    if visual_stats.get('charts_failed', 0) > 0 or visual_stats.get('tables_failed', 0) > 0:
        report_lines.append("   ‚Ä¢ V√©rifier les erreurs ASCII dans les logs")
        report_lines.append("   ‚Ä¢ Consid√©rer r√©duire visual_batch_size")
    
    if total_content > 10000:
        report_lines.append("   ‚Ä¢ Excellent volume de donn√©es index√©es!")
    elif total_content > 1000:
        report_lines.append("   ‚Ä¢ Bon volume de donn√©es index√©es")
    else:
        report_lines.append("   ‚Ä¢ V√©rifier si tous les PDFs ont √©t√© trait√©s")
    
    return "\n".join(report_lines)


# =============================================================================
# FONCTION DE TEST PRINCIPALE
# =============================================================================

def run_ascii_fix_tests():
    """Lance tous les tests de correction ASCII."""
    
    print("üõ†Ô∏è TESTS DE CORRECTION ASCII POUR SUN-STATS")
    print("=" * 60)
    
    # Test de la fonction principale
    test_sanitize_pinecone_id()
    
    # Test de validation
    print("\nüîç TESTS DE VALIDATION:")
    print("-" * 30)
    
    test_ids = [
        "visual_table_Tableau_VII-7_Proportion_de_c√©libataires_d√©finitif_p5_t7",  # Probl√©matique
        "visual_chart_valid_ascii_id",  # Valide
        "a" * 600,  # Trop long
        ""  # Vide
    ]
    
    for test_id in test_ids:
        validation = validate_pinecone_id(test_id)
        status = "‚úÖ VALIDE" if validation['is_valid'] else "‚ùå INVALIDE"
        print(f"{status}: {test_id[:50]}{'...' if len(test_id) > 50 else ''}")
        
        if validation['errors']:
            for error in validation['errors']:
                print(f"    ‚ùå {error}")
        
        if validation['warnings']:
            for warning in validation['warnings']:
                print(f"    ‚ö†Ô∏è {warning}")
    
    print("\n‚úÖ Tests termin√©s. Utilisez sanitize_pinecone_id() dans votre code d'indexation.")


if __name__ == "__main__":
    # Pour tester directement ce fichier
    run_ascii_fix_tests()